{
 "cells": [
  {
   "cell_type": "code",
   "id": "6d18f2c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:21:13.421138Z",
     "start_time": "2026-01-03T14:21:13.413353Z"
    }
   },
   "source": [
    "import json\n",
    "with open(\"instruction-data.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(data[1])\n",
    "print(len(data))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Edit the following sentence for grammar.', 'input': 'He go to the park every day.', 'output': 'He goes to the park every day.'}\n",
      "1100\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "4a7dc1d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:21:13.732787Z",
     "start_time": "2026-01-03T14:21:13.726815Z"
    }
   },
   "source": [
    "def format_input(item):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task.\"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{item['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{item['input']}\" if item[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text\n",
    "\n",
    "myinput = format_input(data[50])\n",
    "response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(myinput+response)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "9e31b52a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:21:15.826699Z",
     "start_time": "2026-01-03T14:21:15.822682Z"
    }
   },
   "source": [
    "#8:1:1\n",
    "train_part = (int)(len(data) *0.8)\n",
    "val_part = (int)(len(data)*0.1)\n",
    "test_part = len(data)-train_part - val_part\n",
    "\n",
    "train_data = data[:train_part]\n",
    "val_data = data[train_part:train_part+val_part]\n",
    "test_data = data[train_part+val_part:]\n",
    "\n",
    "print(\"train set length:\", len(train_data))\n",
    "print(\"val set length:\", len(val_data))\n",
    "print(\"test set length:\", len(test_data))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set length: 880\n",
      "val set length: 110\n",
      "test set length: 110\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:21:08.112686Z",
     "start_time": "2026-01-03T14:21:04.784837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install torch\n",
    "!pip install tiktoken"
   ],
   "id": "ea697038af3595e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: filelock in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from torch) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from torch) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from torch) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from jinja2->torch) (3.0.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Collecting tiktoken\r\n",
      "  Downloading tiktoken-0.12.0-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from tiktoken) (2024.9.11)\r\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from tiktoken) (2.32.3)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\r\n",
      "Downloading tiktoken-0.12.0-cp39-cp39-macosx_10_12_x86_64.whl (1.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.1/1.1 MB\u001B[0m \u001B[31m10.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tiktoken\r\n",
      "Successfully installed tiktoken-0.12.0\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "37b809dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:21:19.128377Z",
     "start_time": "2026-01-03T14:21:18.244997Z"
    }
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.samples = []\n",
    "        for i in data:\n",
    "            input = format_input(i)\n",
    "            response = f\"\\n\\n### Response:\\n{i['output']}\"\n",
    "            full_text = input + response\n",
    "            self.samples.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "13d6ecc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:21:52.544489Z",
     "start_time": "2026-01-03T14:21:21.684165Z"
    }
   },
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "9e0e6815",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:21:52.550280Z",
     "start_time": "2026-01-03T14:21:52.545977Z"
    }
   },
   "source": [
    "def my_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,\n",
    "    ignore_token_id=-100,\n",
    "    allowed_max_length=None,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    #获取这个批次中最长样本的长度\n",
    "    batch_max_len = max(len(i)+1 for i in batch)\n",
    "    #bacth_max_len = max(len(i)+1 for i in batch)\n",
    "    input_list, target_list = [], []\n",
    "\n",
    "    for i in batch:\n",
    "        #将这个批次中小于批次最大长度的所有样本进行填充\n",
    "        #根据输入创建targets\n",
    "        #将targets中填充的token_id替换成-100（除第一个填充的tokenid之外）\n",
    "        new_item = i + [pad_token_id]\n",
    "        padded = new_item + [pad_token_id] * (batch_max_len - len(new_item))\n",
    "        #padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "        inputs = torch.tensor(padded[:-1])\n",
    "        targets = torch.tensor(padded[1:])\n",
    "\n",
    "        # targets = [1 2 3 50256 50256 ...],\n",
    "        # mask = [False, False, False, True, True ...]\n",
    "        mask = targets == pad_token_id\n",
    "        slice = torch.nonzero(mask).squeeze()\n",
    "        if slice.numel() > 1:\n",
    "            targets[slice[1:]] = ignore_token_id\n",
    "        \n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "        \n",
    "        input_list.append(inputs)\n",
    "        target_list.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(input_list).to(device)\n",
    "    #targets_tensor = troch.stack(target_list)\n",
    "    targets_tensor = torch.stack(target_list)\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "cbd31917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:21:52.558857Z",
     "start_time": "2026-01-03T14:21:52.551422Z"
    }
   },
   "source": [
    "import torch\n",
    "mask = [False, False, True, True, True]\n",
    "slice = torch.nonzero(torch.tensor(mask)).squeeze()\n",
    "targets=torch.tensor([1,2,50256, 50256,50256])\n",
    "targets[slice[1:]] = -100\n",
    "print(targets)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1,     2, 50256,  -100,  -100])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "604a59a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:21:52.563416Z",
     "start_time": "2026-01-03T14:21:52.560764Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from functools import partial\n",
    "customized_collate_fn = partial(my_collate_fn, \n",
    "                                device=device,\n",
    "                                allowed_max_length=1024)\n",
    "print(device)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "aa341586",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:21:52.753542Z",
     "start_time": "2026-01-03T14:21:52.564385Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = True,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "#验证数据集\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = batch_size,\n",
    "    collate_fn = customized_collate_fn,\n",
    "    shuffle = False,\n",
    "    drop_last = False\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "f6d10574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:23:38.690796Z",
     "start_time": "2026-01-03T14:22:48.516975Z"
    }
   },
   "source": [
    "from GPTModel import MyGPTModel, generate_new, text_to_tokenids, tokenids_to_text\n",
    "from load_gpt2_model import load_gpt2_weights\n",
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"max_seq_length\": 1024,\n",
    "    \"embedding_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "model = MyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "load_gpt2_weights(model, GPT_CONFIG_124M)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"模型已加载至：{device}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] 正在从 Hugging Face 下载/加载 gpt2 模型...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4735094ac4ef4f0587bea85629c50787"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62ad71cf4b334080922dbabd10227347"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8bb3397acc094f818836d27d4f953268"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/3] 开始权重移植...\n",
      "  -> 正在加载 Embeddings (wte, wpe)...\n",
      "  -> 正在加载 12 层 Transformer Block...\n",
      "  -> 正在加载 Final LayerNorm & Head...\n",
      "[3/3] 成功！GPT-2 权重已全部加载完成。\n",
      "\n",
      "模型已加载至：cpu\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "064aec97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:23:47.740932Z",
     "start_time": "2026-01-03T14:23:45.690727Z"
    }
   },
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "prompt = \"OpenAI is\"\n",
    "prompt = text_to_tokenids(prompt, tokenizer).to(device)\n",
    "tokens = generate_new(model, \n",
    "             prompt,\n",
    "             30,\n",
    "             GPT_CONFIG_124M[\"max_seq_length\"],\n",
    "             25,\n",
    "             1.2)\n",
    "\n",
    "print(f\"output:{tokenids_to_text(tokens, tokenizer)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:OpenAI is a small company that operates out of the office at the University of Minnesota.\n",
      "\n",
      "The company does provide a \"soft\" version for small businesses (\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "id": "54e2be4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-03T14:25:07.093393Z",
     "start_time": "2026-01-03T14:23:52.057996Z"
    }
   },
   "source": [
    "from GPTModel import calc_loss\n",
    "with torch.no_grad():\n",
    "        train_loss = calc_loss(train_dataloader, model, device)\n",
    "        val_loss = calc_loss(val_dataloader, model, device)\n",
    "\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: tensor(4.1874)\n",
      "Validation loss: tensor(4.2892)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "3ea99023",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2026-01-03T14:25:07.095033Z"
    }
   },
   "source": [
    "import time\n",
    "from GPTModel import train_model\n",
    "\n",
    "start_time = time.time()\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00005, weight_decay=0.1)\n",
    "num_epochs = 2\n",
    "\n",
    "train_losses, val_losses = train_model(\n",
    "    model, \n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    optimizer, \n",
    "    device,\n",
    "    epochs=num_epochs, \n",
    "    prompt=format_input(val_data[0]), \n",
    "    tokenizer=tokenizer,\n",
    "    eval_interval=5\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f4282986",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "torch.save(model.state_dict(), \"model_sft.pth\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e29b4210",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "torch.manual_seed(123)\n",
    "for entry in test_data[:3]:                #A\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate_new(model,\n",
    "        text_to_tokenids(input_text, tokenizer).to(device),\n",
    "        256,\n",
    "        GPT_CONFIG_124M[\"max_seq_length\"],\n",
    "        25,\n",
    "        1.0,\n",
    "        50256)\n",
    "    generated_text = tokenids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\",\"\").strip()\n",
    "\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------------------\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7766f1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110/110 [00:19<00:00,  5.68it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from GPTModel import generate_new\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "\n",
    "    token_ids = generate_new(\n",
    "        model=model,\n",
    "        prompt=text_to_tokenids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_seq_size=GPT_CONFIG_124M[\"max_seq_length\"],\n",
    "        eos_id=50256\n",
    "    )\n",
    "    generated_text = tokenids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\",\n",
    "\"\").strip()\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4) # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c7ed0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "\n",
    "def query_model(prompt, model=\"qwen3:latest\", url=\"http://127.0.0.1:11434/api/chat\"):\n",
    "    data = {                                                               \n",
    "        \"model\": model,\n",
    "        \"option\":{\n",
    "            \"seed\": 123, # for deterministic responses\n",
    "            \"temperature\": 0, # for deterministic responses\n",
    "        },\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    payload = json.dumps(data).encode(\"utf-8\")                             \n",
    "    request = urllib.request.Request(url, data=payload, method=\"POST\")     \n",
    "    request.add_header(\"Content-Type\", \"application/json\")                 \n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:                      \n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d90e0907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是通义千问，是阿里巴巴集团旗下的通义实验室自主研发的超大规模语言模型。我能够帮助你回答问题、创作文字、逻辑推理、编程等，也可以进行多轮对话交流。如果你有任何问题或需要帮助，欢迎随时告诉我！\n"
     ]
    }
   ],
   "source": [
    "result = query_model(\"你好，你是谁？\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eea214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The primary function of the human heart is to pump blood throughout the body, delivering oxygen and nutrients to tissues and removing carbon dioxide and other wastes.\n",
      "\n",
      "Model response:\n",
      ">> The primary function of the human heart is to control blood flow through the body and to maintain oxygenation and function. It is the primary function of the heart to maintain blood flow through the body and to maintain oxygenation and function.\n",
      "\n",
      "Score:\n",
      ">> 60\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> He will be reading a novel inspired by his grandmother.\n",
      "\n",
      "Model response:\n",
      ">> He is reading a novel inspired by his grandmother.\n",
      "\n",
      "Score:\n",
      ">> 0\n",
      "\n",
      "-------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The government passed the law.\n",
      "\n",
      "Model response:\n",
      ">> The law was passed by the government.\n",
      "\n",
      "Score:\n",
      ">> 0\n",
      "\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "with open(\"instruction-data-with-response.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    jsondata = json.load(f)\n",
    "#print(data[0])  \n",
    "for entry in jsondata[:3]:\n",
    "    prompt = (\n",
    "        f\" 给定一个输入： `{format_input(entry)}` \"\n",
    "        f\" 正确的输出为: `{entry['output']}`, \"\n",
    "        f\" 模型给的输出为： `{entry['model_response']}`\"\n",
    "        f\" 请为模型的输出打分，0表示最差，100表示最好，只给出分数。\"\n",
    "    )\n",
    "    print(\"\\nDataset response:\")\n",
    "    print(\">>\", entry['output'])\n",
    "    print(\"\\nModel response:\")\n",
    "    print(\">>\", entry[\"model_response\"])\n",
    "    print(\"\\nScore:\")\n",
    "    print(\">>\", query_model(prompt))\n",
    "    print(\"\\n-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f442f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  73%|███████▎  | 80/110 [09:21<45:42, 91.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: 根据用户提供的上下文，模型输出为“Run!”，而用户期望的正确输出为“Run.”。这表明存在一个细微但关键的差异：**标点符号的使用**。\n",
      "\n",
      "### 分析：\n",
      "1. **用户期望的输出**：`Run.`（句号结尾）\n",
      "2. **模型输出**：`Run!`（感叹号结尾）\n",
      "\n",
      "### 可能的原因：\n",
      "- **标点符号错误**：模型可能在生成文本时误用了感叹号（!）而非句号（.），这在某些上下文中（如指令或陈述句）是不合适的。\n",
      "- **上下文理解偏差**：如果用户期望的是一个陈述句（例如“Run.”），而模型生成的是一个命令或强调（如“Run!”），这可能与用户的需求不匹配。\n",
      "\n",
      "### 建议：\n",
      "- **检查上下文**：确认用户是否需要特定的标点符号（如句号或感叹号）。例如：\n",
      "  - 如果是程序指令或命令，`Run!` 可能更符合语气。\n",
      "  - 如果是陈述或描述，`Run.` 可能更合适。\n",
      "- **模型调整**：如果用户明确要求句号，可能需要调整模型的标点生成逻辑，确保其符合上下文需求。\n",
      "- **用户澄清**：如果上下文不明确，建议用户进一步说明需求，以确保输出符合预期。\n",
      "\n",
      "### 总结：\n",
      "模型的输出与用户期望的正确输出存在标点符号差异，需根据具体场景判断是否需要修正。如果用户明确需要句号，则模型应调整生成策略以避免此类错误。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|██████████| 110/110 [11:07<00:00,  6.07s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 21\u001B[0m\n\u001B[0;32m     18\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m scores\n\u001B[0;32m     20\u001B[0m scores \u001B[38;5;241m=\u001B[39m generate_model_scores(jsondata)\n\u001B[1;32m---> 21\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of scores: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(scores)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(\u001B[43mtest_data\u001B[49m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAverage score: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28msum\u001B[39m(scores)\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(scores)\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.2f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def generate_model_scores(json_data):\n",
    "    scores = []\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        prompt = (\n",
    "            f\"给定一个输出： `{format_input(entry)}` \"\n",
    "            f\"正确的输出为: `{entry['output']}`, \"\n",
    "            f\"模型给的输出为： `{entry['model_response']}`\"\n",
    "            f\" 请为模型的输出打分，0表示最差，100表示最好，只给出分数。\"                       \n",
    "        )\n",
    "        score = query_model(prompt)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = generate_model_scores(jsondata)\n",
    "print(f\"Number of scores: {len(scores)} of {len(jsondata)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32cb16a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 109 of 110\n",
      "Average score: 11.33\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of scores: {len(scores)} of {len(jsondata)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
