{
 "cells": [
  {
   "cell_type": "code",
   "id": "59b31544",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:33:17.371242Z",
     "start_time": "2025-12-28T15:33:00.134856Z"
    }
   },
   "source": [
    "import sys\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# ================= 修改配置 =================\n",
    "#TEACHER_MODEL_PATH = os.path.join(BASE_DIR, \"teacher_7b\")\n",
    "#STUDENT_MODEL_PATH = os.path.join(BASE_DIR, \"student_0.5b\")\n",
    "#TEACHER_MODEL_PATH = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "TEACHER_MODEL_PATH = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "STUDENT_MODEL_PATH = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "print(f\"正在从本地加载老师模型: {TEACHER_MODEL_PATH}\")\n",
    "print(f\"正在从本地加载学生模型: {STUDENT_MODEL_PATH}\")\n",
    "\n",
    "# Tokenizer 加载 (直接读本地文件夹)\n",
    "tokenizer_s = AutoTokenizer.from_pretrained(STUDENT_MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer_t = AutoTokenizer.from_pretrained(TEACHER_MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "# 确保 pad_token 存在 (Qwen 有时默认 pad_token 为 None)\n",
    "if tokenizer_s.pad_token is None: tokenizer_s.pad_token = tokenizer_s.eos_token\n",
    "if tokenizer_t.pad_token is None: tokenizer_t.pad_token = tokenizer_t.eos_token\n",
    "\n",
    "# 核心检查：白盒蒸馏要求老师和学生的 Logits 对应同一个词表\n",
    "if tokenizer_s.vocab_size != tokenizer_t.vocab_size:\n",
    "    print(f\"[错误] 词表不匹配！学生: {tokenizer_s.vocab_size}, 老师: {tokenizer_t.vocab_size}\")\n",
    "    print(\"Logits 无法对齐，无法进行白盒蒸馏。程序退出。\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\">>> 验证通过：老师和学生词表一致。\")\n",
    "    tokenizer = tokenizer_s # 后续统一使用一个 tokenizer"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在从本地加载老师模型: Qwen/Qwen2.5-7B-Instruct\n",
      "正在从本地加载学生模型: Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c79c66cab0b34ee4b0c092fbe2632f39"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04426386eedf4feeb982c0eefa34b748"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "09602abc9e10428299ece5bbb2154644"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6928bcdfa0e4f5684402c0b5ef86ed9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "28932ad52df34858a82ca54af362b368"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "945de04bfa894f66b2b30b86569b493c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00f19ba66679446cbbbd596d7267c773"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "13f04c5b311e4803acebc71eeb5b12ef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 验证通过：老师和学生词表一致。\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "38a914de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:37:29.278174Z",
     "start_time": "2025-12-28T15:37:29.269469Z"
    }
   },
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# ================= 配置下载路径 =================\n",
    "# 建议在当前目录下创建一个 models 文件夹，方便管理\n",
    "BASE_DIR = \"./local_models\"\n",
    "\n",
    "# 1. 老师模型 ID (假设是 GPTQ 量化版)\n",
    "TEACHER_ID = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "#TEACHER_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "# 老师模型本地存储路径\n",
    "TEACHER_DIR = os.path.join(BASE_DIR, \"teacher_3_4b\")\n",
    "\n",
    "# 2. 学生模型 ID\n",
    "STUDENT_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# 学生模型本地存储路径\n",
    "STUDENT_DIR = os.path.join(BASE_DIR, \"student_0.5b\")\n",
    "\n",
    "# ================= 开始下载 =================\n",
    "print(f\"准备下载模型到: {BASE_DIR}\")\n",
    "\n",
    "# 下载老师模型\n",
    "if not os.path.exists(TEACHER_DIR):\n",
    "    print(f\"正在下载老师模型: {TEACHER_ID} ...\")\n",
    "    snapshot_download(\n",
    "        repo_id=TEACHER_ID,\n",
    "        local_dir=TEACHER_DIR,\n",
    "        local_dir_use_symlinks=False, # 设为 False 确保下载的是实际文件而不是快捷方式\n",
    "        resume_download=True          # 支持断点续传\n",
    "    )\n",
    "    print(\"老师模型下载完成！\")\n",
    "else:\n",
    "    print(f\"老师模型目录已存在，跳过下载: {TEACHER_DIR}\")\n",
    "\n",
    "# 下载学生模型\n",
    "if not os.path.exists(STUDENT_DIR):\n",
    "    print(f\"正在下载学生模型: {STUDENT_ID} ...\")\n",
    "    snapshot_download(\n",
    "        repo_id=STUDENT_ID,\n",
    "        local_dir=STUDENT_DIR,\n",
    "        local_dir_use_symlinks=False,\n",
    "        resume_download=True\n",
    "    )\n",
    "    print(\"学生模型下载完成！\")\n",
    "else:\n",
    "    print(f\"学生模型目录已存在，跳过下载: {STUDENT_DIR}\")\n",
    "\n",
    "print(\"\\n所有模型准备就绪！\")\n",
    "print(f\"老师模型路径: {os.path.abspath(TEACHER_DIR)}\")\n",
    "print(f\"学生模型路径: {os.path.abspath(STUDENT_DIR)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准备下载模型到: ./local_models\n",
      "老师模型目录已存在，跳过下载: ./local_models/teacher_3_4b\n",
      "学生模型目录已存在，跳过下载: ./local_models/student_0.5b\n",
      "\n",
      "所有模型准备就绪！\n",
      "老师模型路径: /Users/lhc456/Desktop/python/play-with-instruct-model/cot/local_models/teacher_3_4b\n",
      "学生模型路径: /Users/lhc456/Desktop/python/play-with-instruct-model/cot/local_models/student_0.5b\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:38:31.379968Z",
     "start_time": "2025-12-28T15:38:13.355754Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install trl",
   "id": "a6d2031b2fec8aa3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\r\n",
      "  Downloading trl-0.24.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from trl) (1.4.0)\r\n",
      "Requirement already satisfied: datasets>=3.0.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from trl) (3.0.1)\r\n",
      "Collecting transformers>=4.56.1 (from trl)\r\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\r\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from accelerate>=1.4.0->trl) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from accelerate>=1.4.0->trl) (24.1)\r\n",
      "Requirement already satisfied: psutil in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from accelerate>=1.4.0->trl) (6.0.0)\r\n",
      "Requirement already satisfied: pyyaml in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from accelerate>=1.4.0->trl) (6.0.2)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from accelerate>=1.4.0->trl) (2.2.2)\r\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from accelerate>=1.4.0->trl) (0.25.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from accelerate>=1.4.0->trl) (0.4.5)\r\n",
      "Requirement already satisfied: filelock in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (3.16.1)\r\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (17.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (0.3.8)\r\n",
      "Requirement already satisfied: pandas in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (2.2.3)\r\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (4.66.5)\r\n",
      "Requirement already satisfied: xxhash in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (3.5.0)\r\n",
      "Requirement already satisfied: multiprocess in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (0.70.16)\r\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets>=3.0.0->trl) (2024.6.1)\r\n",
      "Requirement already satisfied: aiohttp in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from datasets>=3.0.0->trl) (3.10.10)\r\n",
      "Collecting huggingface-hub>=0.21.0 (from accelerate>=1.4.0->trl)\r\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from transformers>=4.56.1->trl) (2024.9.11)\r\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\r\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from aiohttp->datasets>=3.0.0->trl) (2.4.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from aiohttp->datasets>=3.0.0->trl) (1.3.1)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from aiohttp->datasets>=3.0.0->trl) (24.2.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from aiohttp->datasets>=3.0.0->trl) (1.4.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from aiohttp->datasets>=3.0.0->trl) (6.1.0)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from aiohttp->datasets>=3.0.0->trl) (1.15.2)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from aiohttp->datasets>=3.0.0->trl) (4.0.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from huggingface-hub>=0.21.0->accelerate>=1.4.0->trl) (4.12.2)\r\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.21.0->accelerate>=1.4.0->trl)\r\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl.metadata (4.9 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.2.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2024.8.30)\r\n",
      "Requirement already satisfied: sympy in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.2.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from torch>=2.0.0->accelerate>=1.4.0->trl) (3.1.4)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from pandas->datasets>=3.0.0->trl) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from pandas->datasets>=3.0.0->trl) (2024.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.16.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=3.0.0->trl) (0.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from jinja2->torch>=2.0.0->accelerate>=1.4.0->trl) (3.0.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages (from sympy->torch>=2.0.0->accelerate>=1.4.0->trl) (1.3.0)\r\n",
      "Downloading trl-0.24.0-py3-none-any.whl (423 kB)\r\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.0/12.0 MB\u001B[0m \u001B[31m17.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m566.1/566.1 kB\u001B[0m \u001B[31m25.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl (3.1 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m3.1/3.1 MB\u001B[0m \u001B[31m72.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading hf_xet-1.2.0-cp37-abi3-macosx_10_12_x86_64.whl (2.9 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.9/2.9 MB\u001B[0m \u001B[31m78.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: hf-xet, huggingface-hub, tokenizers, transformers, trl\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.25.2\r\n",
      "    Uninstalling huggingface-hub-0.25.2:\r\n",
      "      Successfully uninstalled huggingface-hub-0.25.2\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.20.1\r\n",
      "    Uninstalling tokenizers-0.20.1:\r\n",
      "      Successfully uninstalled tokenizers-0.20.1\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.2\r\n",
      "    Uninstalling transformers-4.45.2:\r\n",
      "      Successfully uninstalled transformers-4.45.2\r\n",
      "Successfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 tokenizers-0.22.1 transformers-4.57.3 trl-0.24.0\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "a85e73cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:43:09.074663Z",
     "start_time": "2025-12-28T15:43:09.030699Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    Trainer, \n",
    "    TrainingArguments, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer\n",
    ")\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. 配置参数\n",
    "# ==============================================================================\n",
    "BASE_DIR = \"./local_models\"\n",
    "\n",
    "TEACHER_MODEL_PATH = os.path.join(BASE_DIR, \"teacher_7b\")\n",
    "STUDENT_MODEL_PATH = os.path.join(BASE_DIR, \"student_0.5b\")\n",
    "#TEACHER_MODEL_PATH = \"Qwen/Qwen2.5-7B-Instruct-GPTQ-Int4\" \n",
    "#STUDENT_MODEL_PATH = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "DATA_FILE = \"math_dataset_with_answers_new.jsonl\" \n",
    "\n",
    "# 训练超参数\n",
    "MAX_LENGTH = 1024  \n",
    "\n",
    "# TEMPERATURE (温度 T)的作用：\n",
    "# 原始情况：输出的 Logits 可能是 [猫: 10, 狗: 2, 汽车: -5]。\n",
    "# 经过 Softmax 后，变成 [0.999, 0.001, 0.0]。这样“狗”和“汽车”的区别被抹平了\n",
    "# 但白盒蒸馏，就是想把老师模型中，这些差别学到。\n",
    "# 所以把 Logits 除以一个温度（比如 T=2.0）。数值变成 [5, 1, -2.5]。Softmax 后可能变成 [0.8, 0.15, 0.05]。\n",
    "# 效果：概率分布变“平缓”了。 这样“狗(0.15)”和“汽车(0.05)”的区别就显现出来了，学生就能学到这些细节。\n",
    "TEMPERATURE = 2.0  \n",
    " \n",
    "# ALPHA (比如 0.5)：\n",
    "#   0.5 份的努力：去对齐标准答案（做对题）。\n",
    "#   0.5 份的努力：去模仿老师的思考方式（学气质）。\n",
    "#  Loss Total=α⋅Loss_CE+(1−α)⋅Loss_KD\n",
    "ALPHA = 0.5 \n",
    "\n",
    "BATCH_SIZE = 2           \n",
    "GRAD_ACCUMULATION = 4    \n",
    "EPOCHS = 3               \n",
    "\n",
    "# ==============================================================================\n",
    "# 2. 加载与校验 Tokenizer\n",
    "# ==============================================================================\n",
    "print(\">>> 正在校验 Tokenizer...\")\n",
    "# 这里的 trust_remote_code=True 是必须的，防止某些自定义模型报错\n",
    "tokenizer_s = AutoTokenizer.from_pretrained(STUDENT_MODEL_PATH, trust_remote_code=True)\n",
    "tokenizer_t = AutoTokenizer.from_pretrained(TEACHER_MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "if tokenizer_s.pad_token is None: tokenizer_s.pad_token = tokenizer_s.eos_token\n",
    "if tokenizer_t.pad_token is None: tokenizer_t.pad_token = tokenizer_t.eos_token\n",
    "\n",
    "# 校验逻辑\n",
    "if tokenizer_s.vocab_size != tokenizer_t.vocab_size:\n",
    "    print(f\"[错误] 词表不匹配！学生: {tokenizer_s.vocab_size}, 老师: {tokenizer_t.vocab_size}\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(f\">>> 验证通过：逻辑词表大小一致 ({tokenizer_s.vocab_size})。\")\n",
    "    tokenizer = tokenizer_s \n",
    "\n",
    "# ==============================================================================\n",
    "# 3. 数据处理\n",
    "# ==============================================================================\n",
    "print(f\">>> 正在读取数据: {DATA_FILE}\")\n",
    "dataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\n",
    "\n",
    "def process_func(example):\n",
    "    # 1. 拼接文本\n",
    "    combined_content = f\"[思考过程]\\n{example['thinking']}\\n\\n[最终答案]\\n{example['output']}\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": example['instruction'] + (\"\\n\" + example['input'] if example.get('input') else \"\")},\n",
    "        {\"role\": \"assistant\", \"content\": combined_content}\n",
    "    ]\n",
    "    # 2. 转字符串\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    # 3. 转数字\n",
    "    tokenized = tokenizer(text, max_length=MAX_LENGTH, truncation=True)\n",
    "    return tokenized\n",
    "\n",
    "print(\">>> 处理数据中 (Tokenizing)...\")\n",
    "tokenized_dataset = dataset.map(process_func, remove_columns=dataset.column_names)\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Collator 设置\n",
    "# ==============================================================================\n",
    "response_template = \"<|im_start|>assistant\\n\" \n",
    "collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template,\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False \n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. 加载模型 (核心修改部分)\n",
    "# ==============================================================================\n",
    "print(\">>> 加载 Teacher (Int4)...\")\n",
    "# 老师不训练，可以是 FP16 甚至 Int4，没问题\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(\n",
    "    TEACHER_MODEL_PATH,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float16, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "teacher_model.eval()\n",
    "for param in teacher_model.parameters(): param.requires_grad = False\n",
    "\n",
    "print(\">>> 加载 Student (FP32)...\")\n",
    "# 【核心修复！！！】\n",
    "# 学生模型必须用 float32 加载，或者不写 torch_dtype (默认就是 float32)\n",
    "# 这样 Trainer(fp16=True) 才能正常建立混合精度的 Master Weights\n",
    "student_model = AutoModelForCausalLM.from_pretrained(\n",
    "    STUDENT_MODEL_PATH,\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=torch.float32,  # <--- 改为 float32，解决 unscale 报错\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# 6. 自定义 Trainer (带维度对齐)\n",
    "# ==============================================================================\n",
    "class KDTrainer(Trainer):\n",
    "    def __init__(self, teacher_model, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # 1. 学生前向\n",
    "        student_outputs = model(**inputs)\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        # 2. 老师前向\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "        \n",
    "        # 3. 【维度对齐】解决 152064 vs 151936 报错\n",
    "        if student_logits.shape[-1] != teacher_logits.shape[-1]:\n",
    "            min_dim = min(student_logits.shape[-1], teacher_logits.shape[-1])\n",
    "            student_logits = student_logits[..., :min_dim]\n",
    "            teacher_logits = teacher_logits[..., :min_dim]\n",
    "        \n",
    "        # 4. 计算 KD Loss\n",
    "\n",
    "        # KL 散度：这是用来衡量两个概率分布（学生预测的分布 vs 老师预测的分布）有多大差异的数学工具。\n",
    "        # 目标：让学生的概率分布尽可能趋近于老师的分布。\n",
    "        loss_fct = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "\n",
    "        # 问题1：为什么学生是 log_softmax 而老师是 softmax？\n",
    "        # 这是 PyTorch nn.KLDivLoss 接口的特殊要求：\n",
    "        #   输入（Input）：要求是 Log 概率（所以学生用 log_softmax）。\n",
    "        #   目标（Target）：要求是普通概率（所以老师用 softmax）。\n",
    "        \n",
    "        # 问题2：为什么要乘以 (TEMPERATURE ** 2)？\n",
    "        # 这是一个梯度补偿机制。当你把 Logits 除以T时，反向传播计算出来的梯度数值会缩小 1/T^2 倍。\n",
    "        # 为了让 KD Loss 的梯度大小和 CE Loss 在同一个数量级，不让它变得太小而被忽略，所以人为乘回T^2\n",
    "        loss_kd = loss_fct(\n",
    "            F.log_softmax(student_logits / TEMPERATURE, dim=-1),\n",
    "            F.softmax(teacher_logits / TEMPERATURE, dim=-1)\n",
    "        ) * (TEMPERATURE ** 2)\n",
    "        \n",
    "        # CE Loss让学生学会：这道题答案是 \"12\"。\n",
    "        # KD Loss让学生学会：为什么老师在算出 \"12\" 之前，对 \"10\" 或 \"15\" 也有过一点点概率的侧重（可能是中间步骤的近似）。\n",
    "        loss_ce = student_outputs.loss #普通的交叉熵损失，即答案的损失\n",
    "        total_loss = ALPHA * loss_ce + (1 - ALPHA) * loss_kd\n",
    "        \n",
    "        return (total_loss, student_outputs) if return_outputs else total_loss\n",
    "\n",
    "# ==============================================================================\n",
    "# 7. 训练\n",
    "# ==============================================================================\n",
    "#所谓混合精度就是计算时用高精度，存放到显存中的用低精度\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_math_distilled\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACCUMULATION,\n",
    "    learning_rate=2e-5,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    fp16=True,                # 开启混合精度，此时需要 Student 是 FP32， 省显存、提速度、防溢出。\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=True\n",
    ")\n",
    "\n",
    "trainer = KDTrainer(\n",
    "    teacher_model=teacher_model,\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "print(\">>> 开始训练...\")\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()\n",
    "\n",
    "print(\">>> 保存模型...\")\n",
    "trainer.save_model(\"./final_math_student_model\")\n",
    "print(\"完成！\")"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataCollatorForCompletionOnlyLM' from 'trl' (/Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/trl/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 12\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      7\u001B[0m     Trainer, \n\u001B[1;32m      8\u001B[0m     TrainingArguments, \n\u001B[1;32m      9\u001B[0m     AutoModelForCausalLM, \n\u001B[1;32m     10\u001B[0m     AutoTokenizer\n\u001B[1;32m     11\u001B[0m )\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtrl\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DataCollatorForCompletionOnlyLM\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# 1. 配置参数\u001B[39;00m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# ==============================================================================\u001B[39;00m\n\u001B[1;32m     17\u001B[0m BASE_DIR \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./local_models\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'DataCollatorForCompletionOnlyLM' from 'trl' (/Users/lhc456/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/trl/__init__.py)"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "e70a9474",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T15:46:14.781365Z",
     "start_time": "2025-12-28T15:46:14.671207Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ================= 配置 =================\n",
    "# 指向刚才保存的微调后的模型目录\n",
    "MODEL_PATH = \"./final_math_student_model\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"正在加载模型: {MODEL_PATH} ...\")\n",
    "\n",
    "# 1. 加载模型和分词器\n",
    "# 注意：这里加载的是你训练好的 Student，不需要再加载 Teacher 了\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "model.eval() # 切换到评估模式\n",
    "\n",
    "# 2. 定义推理函数\n",
    "def generate_answer(instruction, input_text=\"\"):\n",
    "    # 构造与训练时完全一致的 Prompt 格式\n",
    "    # 注意：训练时我们把 input 拼在了 instruction 后面\n",
    "    user_content = instruction + (\"\\n\" + input_text if input_text else \"\")\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": user_content}\n",
    "    ]\n",
    "    \n",
    "    # 应用聊天模板 (只生成到 user 结束，后面让模型续写 assistant)\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True # 这一步会自动加上 <|im_start|>assistant\\n\n",
    "    )\n",
    "    \n",
    "    # 转为数字\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # 开始生成\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,      # 允许生成的最大长度\n",
    "            do_sample=False,          # 采样模式，稍微有点随机性，更像人说话\n",
    "            temperature=0.2,         # 控制创造性\n",
    "            top_p=0.95,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    # 解码 (只看生成的 Assistant 部分)\n",
    "    # outputs[0] 包含了输入的 prompt + 生成的 response，我们需要切片\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    final_output = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    return final_output\n",
    "\n",
    "# ================= 测试案例 =================\n",
    "print(\"\\n>>> 开始测试 (请观察是否包含 [思考过程])\\n\")\n",
    "\n",
    "# 测试题 1：训练集中类似的题目（看过拟合/记忆能力）\n",
    "q1 = \"计算：24.6 ÷ 0.6\"\n",
    "print(f\"问题: {q1}\")\n",
    "print(\"-\" * 30)\n",
    "ans1 = generate_answer(q1)\n",
    "print(ans1)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 测试题 2：一道新题目（看泛化能力）\n",
    "q2 = \"小明有 5 个苹果，小红的苹果数量是小明的 3 倍少 2 个。请问小红有多少个苹果？\"\n",
    "print(f\"\\n问题: {q2}\")\n",
    "print(\"-\" * 30)\n",
    "ans2 = generate_answer(q2)\n",
    "print(ans2)\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 测试题 3：更难一点的逻辑\n",
    "q3 = \"如果 3x + 5 = 20，那么 x 等于多少？\"\n",
    "print(f\"\\n问题: {q3}\")\n",
    "print(\"-\" * 30)\n",
    "ans3 = generate_answer(q3)\n",
    "print(ans3)\n",
    "print(\"=\" * 50)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型: ./final_math_student_model ...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Incorrect path_or_model_id: './final_math_student_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHFValidationError\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/transformers/utils/hub.py:403\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    401\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;66;03m# Load from URL or cache if already cached\u001B[39;00m\n\u001B[0;32m--> 403\u001B[0m     resolved_file \u001B[38;5;241m=\u001B[39m \u001B[43mhf_hub_download\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath_or_repo_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    406\u001B[0m \u001B[43m        \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrepo_type\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrepo_type\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[43muser_agent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muser_agent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m GatedRepoError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:101\u001B[0m, in \u001B[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    100\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:106\u001B[0m, in \u001B[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m arg_name \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrepo_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfrom_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mto_id\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m--> 106\u001B[0m     \u001B[43mvalidate_repo_id\u001B[49m\u001B[43m(\u001B[49m\u001B[43marg_value\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m arg_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtoken\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m arg_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:160\u001B[0m, in \u001B[0;36mvalidate_repo_id\u001B[0;34m(repo_id)\u001B[0m\n\u001B[1;32m    159\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m REPO_ID_REGEX\u001B[38;5;241m.\u001B[39mmatch(repo_id):\n\u001B[0;32m--> 160\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HFValidationError(\n\u001B[1;32m    161\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRepo id must use alphanumeric chars or \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m are\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    162\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m forbidden, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m and \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m cannot start or end the name, max length is 96:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    163\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mrepo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    164\u001B[0m     )\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m--\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m repo_id \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m..\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m repo_id:\n",
      "\u001B[0;31mHFValidationError\u001B[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './final_math_student_model'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 13\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m正在加载模型: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mMODEL_PATH\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# 1. 加载模型和分词器\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# 注意：这里加载的是你训练好的 Student，不需要再加载 Teacher 了\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mAutoTokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\u001B[43mMODEL_PATH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForCausalLM\u001B[38;5;241m.\u001B[39mfrom_pretrained(\n\u001B[1;32m     15\u001B[0m     MODEL_PATH,\n\u001B[1;32m     16\u001B[0m     device_map\u001B[38;5;241m=\u001B[39mdevice,\n\u001B[1;32m     17\u001B[0m     torch_dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat16\n\u001B[1;32m     18\u001B[0m )\n\u001B[1;32m     19\u001B[0m model\u001B[38;5;241m.\u001B[39meval() \u001B[38;5;66;03m# 切换到评估模式\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:844\u001B[0m, in \u001B[0;36mAutoTokenizer.from_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001B[0m\n\u001B[1;32m    841\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer_class\u001B[38;5;241m.\u001B[39mfrom_pretrained(pretrained_model_name_or_path, \u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    843\u001B[0m \u001B[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001B[39;00m\n\u001B[0;32m--> 844\u001B[0m tokenizer_config \u001B[38;5;241m=\u001B[39m \u001B[43mget_tokenizer_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    845\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m tokenizer_config:\n\u001B[1;32m    846\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m tokenizer_config[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:676\u001B[0m, in \u001B[0;36mget_tokenizer_config\u001B[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001B[0m\n\u001B[1;32m    673\u001B[0m     token \u001B[38;5;241m=\u001B[39m use_auth_token\n\u001B[1;32m    675\u001B[0m commit_hash \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_commit_hash\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 676\u001B[0m resolved_config_file \u001B[38;5;241m=\u001B[39m \u001B[43mcached_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m    \u001B[49m\u001B[43mTOKENIZER_CONFIG_FILE\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    680\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresume_download\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresume_download\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    682\u001B[0m \u001B[43m    \u001B[49m\u001B[43mproxies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mproxies\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    683\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    684\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrevision\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrevision\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    685\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlocal_files_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlocal_files_only\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    686\u001B[0m \u001B[43m    \u001B[49m\u001B[43msubfolder\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msubfolder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    687\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_gated_repo\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    688\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_missing_entries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    689\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_raise_exceptions_for_connection_errors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    690\u001B[0m \u001B[43m    \u001B[49m\u001B[43m_commit_hash\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcommit_hash\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    691\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m resolved_config_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    693\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/transformers/utils/hub.py:469\u001B[0m, in \u001B[0;36mcached_file\u001B[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001B[0m\n\u001B[1;32m    467\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThere was a specific connection error when trying to load \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00merr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    468\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HFValidationError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 469\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mEnvironmentError\u001B[39;00m(\n\u001B[1;32m    470\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncorrect path_or_model_id: \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath_or_repo_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Please provide either the path to a local folder or the repo_id of a model on the Hub.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    471\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resolved_file\n",
      "\u001B[0;31mOSError\u001B[0m: Incorrect path_or_model_id: './final_math_student_model'. Please provide either the path to a local folder or the repo_id of a model on the Hub."
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0c6442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 正在初始化环境，准备加载两个模型进行 PK...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0b0feb24-58fa-41b1-8a90-292cb30dfe59)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Base Model: Qwen/Qwen2.5-0.5B-Instruct ...\n",
      "Loading Distilled Model: ./final_math_student_model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> 模型加载完毕！开始对比测试...\n",
      "\n",
      "\n",
      "📝 测试题目 1: 计算：24.6 ÷ 0.6\n",
      "================================================================================\n",
      "🔴 [原始 0.5B 模型] 回答：\n",
      "要计算 \\(24.6 \\div 0.6\\)，我们可以将除法转换为乘法。具体步骤如下：\n",
      "\n",
      "\\[24.6 \\div 0.6 = 24.6 \\times \\frac{1}{0.6}\\]\n",
      "\n",
      "由于 \\(0.6 = \\frac{3}{5}\\)，我们有：\n",
      "\n",
      "\\[24.6 \\times \\frac{1}{\\frac{3}{5}} = 24.6 \\times \\frac{5}{3}\\]\n",
      "\n",
      "现在，我们将分数相乘：\n",
      "\n",
      "\\[= 8.2 \\times 5\\]\n",
      "\n",
      "\\[= 41\\]\n",
      "\n",
      "因此，\\(24.6 \\div 0.6 = 41\\)。\n",
      "----------------------------------------\n",
      "🟢 [蒸馏后 0.5B 模型] 回答：\n",
      "要计算24.6除以0.6，可以将这个除法问题简化为两个数相除的问题。具体步骤如下：\n",
      "\n",
      "1. 将24.6和0.6都转换成相同的小数位数来方便计算。\n",
      "2. 计算24.6除以0.6。\n",
      "\n",
      "首先，我们可以直接进行计算，也可以先将24.6和0.6转换为更简单的形式，以便于计算。但在这里，我们直接使用24.6除以0.6来进行计算。\n",
      "\n",
      "\\[24.6 ÷ 0.6 = 41\\]\n",
      "\n",
      "因此，24.6除以0.6的结果是41。\n",
      "\n",
      "最终答案是41。\n",
      "================================================================================\n",
      "\n",
      "📝 测试题目 2: 小明有 5 个苹果，小红的苹果数量是小明的 3 倍少 2 个。请问小红有多少个苹果？\n",
      "================================================================================\n",
      "🔴 [原始 0.5B 模型] 回答：\n",
      "小红的数量是小明的 3 倍少 2 个，所以我们可以这样计算：\n",
      "\n",
      "小红的数量 = 小明的数量 - (小明的数量的 3 倍) + 2\n",
      "\n",
      "将小明的数量代入公式中得到：\n",
      "小红的数量 = 5 - (5 * 3) + 2\n",
      "小红的数量 = 5 - 15 + 2\n",
      "小红的数量 = -10 + 2\n",
      "小红的数量 = -8\n",
      "\n",
      "但是这个结果显然是不可能的，因为数量不能为负数。这可能是因为题目中的条件没有完全满足，或者我们对问题的理解存在偏差。\n",
      "\n",
      "如果问题是问的是小红实际拥有的苹果数量，那么答案应该是小红比小明多出的苹果数量加上小明原有的苹果数量。即：\n",
      "- 小红的数量 = 小明的数量 + (小明的数量的 3 倍)\n",
      "- 小红的数量 = 5 + (5 * 3)\n",
      "- 小红的数量 = 5 + 15\n",
      "- 小红的数量 = 20\n",
      "\n",
      "因此，小红实际上有 20 个苹果。\n",
      "----------------------------------------\n",
      "🟢 [蒸馏后 0.5B 模型] 回答：\n",
      "小明有的苹果数为5个。\n",
      "\n",
      "根据题目信息，小红的苹果数量是小明的3倍少2个。可以将这个关系表达为：\n",
      "\\[ 小红的苹果数 = 3 * 小明的苹果数 - 2 \\]\n",
      "\n",
      "代入小明有的苹果数5个进行计算：\n",
      "\n",
      "\\[ 小红的苹果数 = 3 * 5 - 2 \\]\n",
      "\\[ 小红的苹果数 = 15 - 2 \\]\n",
      "\\[ 小红的苹果数 = 13 \\]\n",
      "\n",
      "因此，小红有13个苹果。\n",
      "================================================================================\n",
      "\n",
      "📝 测试题目 3: 如果 3x + 5 = 20，那么 x 等于多少？\n",
      "================================================================================\n",
      "🔴 [原始 0.5B 模型] 回答：\n",
      "要解这个方程 \\(3x + 5 = 20\\)，我们首先需要将等式两边的常数项移到等式的另一边。为此，我们可以从两边减去5：\n",
      "\n",
      "\\[3x + 5 - 5 = 20 - 5\\]\n",
      "\n",
      "这简化为：\n",
      "\n",
      "\\[3x = 15\\]\n",
      "\n",
      "然后，为了求出 \\(x\\) 的值，我们需要将等式两边都除以3：\n",
      "\n",
      "\\[\\frac{3x}{3} = \\frac{15}{3}\\]\n",
      "\n",
      "这简化为：\n",
      "\n",
      "\\[x = 5\\]\n",
      "\n",
      "所以，\\(x\\) 等于5。\n",
      "----------------------------------------\n",
      "🟢 [蒸馏后 0.5B 模型] 回答：\n",
      "要解这个方程 \\(3x + 5 = 20\\)，我们首先需要将等式两边的常数项移到一边，使得等式的两边都为0。这样可以得到一个简单的方程，便于求解。\n",
      "\n",
      "1. 将等式两边同时减去5：\n",
      "\\[3x = 15\\]\n",
      "\n",
      "2. 再将等式两边同时除以3（因为等式两边都乘以相同的数，所以等式两边同时除以同一个数会保持不变）:\n",
      "\\[x = \\frac{15}{3}\\]\n",
      "\n",
      "3. 进行计算得到：\n",
      "\\[x = 5\\]\n",
      "\n",
      "因此，x 的值是 5。\n",
      "\n",
      "最终答案是：\\(x = 5\\)。\n",
      "================================================================================\n",
      "\n",
      "📝 测试题目 4: 你好，请介绍一下你自己。\n",
      "================================================================================\n",
      "🔴 [原始 0.5B 模型] 回答：\n",
      "您好！我是一个人工智能助手，由阿里云开发的模型。我是根据大量的数据训练出来的语言模型，可以回答各种问题、提供信息和帮助用户解决问题。如果您有任何问题或需要帮助，请随时告诉我，我会尽力为您提供支持。\n",
      "----------------------------------------\n",
      "🟢 [蒸馏后 0.5B 模型] 回答：\n",
      "我是一个AI，可以被描述为一个智能助手。我能够回答问题、提供信息和进行对话。我的目标是帮助用户找到所需的信息或解决问题。\n",
      "\n",
      "请告诉我，你最近在做什么？\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# ================= 配置 =================\n",
    "# 原始基座模型 ID\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "# 训练好的模型路径\n",
    "DISTILLED_MODEL_PATH = \"./final_math_student_model\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\">>> 正在初始化环境，准备加载两个模型进行 PK...\")\n",
    "\n",
    "# ================= 1. 加载分词器 =================\n",
    "# 两个模型用的分词器是一样的，加载其中一个即可\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "# ================= 2. 加载 原始基座模型 (Old) =================\n",
    "print(f\"Loading Base Model: {BASE_MODEL_ID} ...\")\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True # 加速加载，减少内存占用\n",
    ")\n",
    "model_base.eval()\n",
    "\n",
    "# ================= 3. 加载 蒸馏后模型 (New) =================\n",
    "print(f\"Loading Distilled Model: {DISTILLED_MODEL_PATH} ...\")\n",
    "model_distilled = AutoModelForCausalLM.from_pretrained(\n",
    "    DISTILLED_MODEL_PATH,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "model_distilled.eval()\n",
    "\n",
    "print(\"\\n>>> 模型加载完毕！开始对比测试...\\n\")\n",
    "\n",
    "# ================= 4. 定义通用的推理函数 =================\n",
    "def get_response(model, question):\n",
    "    \"\"\"\n",
    "    输入模型和问题，返回回答。\n",
    "    使用贪婪搜索 (do_sample=False)，确保输出模型认为概率最大的答案，减少随机性。\n",
    "    \"\"\"\n",
    "    # 构造 Prompt\n",
    "    # 注意：我们使用标准的 User/Assistant 格式\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False, # 【重要】关闭采样，看模型的真实实力，不让它瞎猜\n",
    "            temperature=None, # 关闭采样后，temperature 无效，设为 None\n",
    "            top_p=None,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "    # 解码\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# ================= 5. 测试案例与对比循环 =================\n",
    "\n",
    "questions = [\n",
    "    # 1. 训练集类似的题目（看是否学会了格式）\n",
    "    \"计算：24.6 ÷ 0.6\",\n",
    "    \n",
    "    # 2. 简单的应用题（看逻辑是否变强）\n",
    "    \"小明有 5 个苹果，小红的苹果数量是小明的 3 倍少 2 个。请问小红有多少个苹果？\",\n",
    "    \n",
    "    # 3. 逻辑推理题（看是否能分步思考）\n",
    "    \"如果 3x + 5 = 20，那么 x 等于多少？\",\n",
    "    \n",
    "    # 4. 常识/非数学题（看是否灾难性遗忘，即学了数学忘了怎么说话）\n",
    "    \"你好，请介绍一下你自己。\"\n",
    "]\n",
    "\n",
    "separator = \"=\" * 80\n",
    "\n",
    "for i, q in enumerate(questions):\n",
    "    print(f\"\\n📝 测试题目 {i+1}: {q}\")\n",
    "    print(separator)\n",
    "    \n",
    "    # --- 原始模型回答 ---\n",
    "    print(\"🔴 [原始 0.5B 模型] 回答：\")\n",
    "    try:\n",
    "        ans_base = get_response(model_base, q)\n",
    "        print(ans_base.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    print(\"-\" * 40) # 分割线\n",
    "    \n",
    "    # --- 蒸馏模型回答 ---\n",
    "    print(\"🟢 [蒸馏后 0.5B 模型] 回答：\")\n",
    "    try:\n",
    "        ans_distilled = get_response(model_distilled, q)\n",
    "        print(ans_distilled.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        \n",
    "    print(separator)\n",
    "    # 稍微停顿一下，防止刷新太快\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad9e384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 正在初始化环境...\n",
      "Loading Base Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The module name  (originally ) is not a valid Python identifier. Please rename the original module to avoid import issues.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Distilled Model...\n",
      "\n",
      "📝 测试题目 1: 计算：24.6 ÷ 0.6\n",
      "================================================================================\n",
      "🔴 [原始 0.5B 模型] 回答：\n",
      "要计算 \\(24.6 \\div 0.6\\)，我们可以将除法转换为乘法。具体步骤如下：\n",
      "\n",
      "\\[24.6 \\div 0.6 = 24.6 \\times \\frac{1}{0.6}\\]\n",
      "\n",
      "由于 \\(0.6 = \\frac{3}{5}\\)，我们有：\n",
      "\n",
      "\\[24.6 \\times \\frac{1}{\\frac{3}{5}} = 24.6 \\times \\frac{5}{3}\\]\n",
      "\n",
      "现在，我们将分数相乘：\n",
      "\n",
      "\\[= 8.2 \\times 5\\]\n",
      "\n",
      "\\[= 41\\]\n",
      "\n",
      "因此，\\(24.6 \\div 0.6 = 41\\)。\n",
      "----------------------------------------\n",
      "🟢 [蒸馏后 0.5B 模型] (触发思考模式)：\n",
      "thinking[思考过程]\n",
      "要计算24.6除以0.6，可以将这个除法问题转化为乘法问题。具体来说，就是求解24.6乘以0.6的结果。\n",
      "\n",
      "\\[24.6 ÷ 0.6 = 24.6 × 0.6\\]\n",
      "\n",
      "我们来逐步进行计算：\n",
      "\n",
      "1. 首先，我们可以直接相乘：\n",
      "\\[24.6 × 0.6 = 14.16\\]\n",
      "\n",
      "或者，也可以将24.6和0.6分别转换为分数形式，然后进行乘法运算：\n",
      "\n",
      "\\[24.6 = \\frac{246}{10}，0.6 = \\frac{6}{10}\\]\n",
      "\n",
      "因此，\\( \\frac{246}{10} × \\frac{6}{10} = \\frac{246×6}{10×10} = \\frac{1416}{100} = 14.16 \\)\n",
      "\n",
      "所以，24.6除以0.6的最终结果是14.16。\n",
      "\n",
      "\\[24.6 ÷ 0.6 = 14.16\\]\n",
      "================================================================================\n",
      "\n",
      "📝 测试题目 2: 小明有 5 个苹果，小红的苹果数量是小明的 3 倍少 2 个。请问小红有多少个苹果？\n",
      "================================================================================\n",
      "🔴 [原始 0.5B 模型] 回答：\n",
      "小红的数量是小明的 3 倍少 2 个，所以我们可以这样计算：\n",
      "\n",
      "小红的数量 = 小明的数量 - (小明的数量的 3 倍) + 2\n",
      "\n",
      "将小明的数量代入公式中得到：\n",
      "小红的数量 = 5 - (5 * 3) + 2\n",
      "小红的数量 = 5 - 15 + 2\n",
      "小红的数量 = -10 + 2\n",
      "小红的数量 = -8\n",
      "\n",
      "但是这个结果显然是不可能的，因为数量不能为负数。这可能是因为题目中的条件没有完全满足，或者我们对问题的理解存在偏差。\n",
      "\n",
      "如果问题是问的是小红实际拥有的苹果数量，那么答案应该是小红比小明多出的苹果数量加上小明原有的苹果数量。即：\n",
      "- 小红的数量 = 小明的数量 + (小明的数量的 3 倍)\n",
      "- 小红的数量 = 5 + (5 * 3)\n",
      "- 小红的数量 = 5 + 15\n",
      "- 小红的数量 = 20\n",
      "\n",
      "因此，小红实际上有 20 个苹果。\n",
      "----------------------------------------\n",
      "🟢 [蒸馏后 0.5B 模型] (触发思考模式)：\n",
      "thinking[思考过程]\n",
      "1. 小明有的苹果数为 5 个。\n",
      "2. 小红的苹果数量是小明的 3 倍少 2 个。\n",
      "\n",
      "根据题目信息，我们可以将小红的苹果数量表示为 \\(5 \\times 3 - 2\\)。\n",
      "\n",
      "\\[5 \\times 3 = 15\\]  \n",
      "\\(15 - 2 = 13\\)\n",
      "\n",
      "因此，小红有 13 个苹果。\n",
      "\n",
      "[最终答案]\n",
      "小红有 13 个苹果。\n",
      "================================================================================\n",
      "\n",
      "📝 测试题目 3: 如果 3x + 5 = 20，那么 x 等于多少？\n",
      "================================================================================\n",
      "🔴 [原始 0.5B 模型] 回答：\n",
      "要解这个方程 \\(3x + 5 = 20\\)，我们首先需要将等式两边的常数项移到等式的另一边。为此，我们可以从两边减去5：\n",
      "\n",
      "\\[3x + 5 - 5 = 20 - 5\\]\n",
      "\n",
      "这简化为：\n",
      "\n",
      "\\[3x = 15\\]\n",
      "\n",
      "然后，为了求出 \\(x\\) 的值，我们需要将等式两边都除以3：\n",
      "\n",
      "\\[\\frac{3x}{3} = \\frac{15}{3}\\]\n",
      "\n",
      "这简化为：\n",
      "\n",
      "\\[x = 5\\]\n",
      "\n",
      "所以，\\(x\\) 等于5。\n",
      "----------------------------------------\n",
      "🟢 [蒸馏后 0.5B 模型] (触发思考模式)：\n",
      "thinking[思考过程]\n",
      "给定的方程是 \\(3x + 5 = 20\\). 我们需要解这个方程找出未知数 \\(x\\) 的值。\n",
      "\n",
      "1. 首先将等式两边同时减去5，以使方程简化：\n",
      "\\[3x = 15\\]\n",
      "\n",
      "2. 接着，为了求出 \\(x\\) 的值，我们需要将等式两边同时除以3（因为等式两边都乘以相同的数，即3，可以保持不变）:\n",
      "\\[x = \\frac{15}{3}\\]\n",
      "\n",
      "3. 进行计算得到：\n",
      "\\[x = 5\\]\n",
      "\n",
      "因此，\\(x = 5\\)。\n",
      "\n",
      "最终答案是 \\(x = 5\\)。\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "# ================= 配置 =================\n",
    "BASE_MODEL_ID = \"local_models/student_0.5b/\"\n",
    "DISTILLED_MODEL_PATH = \"./final_math_student_model\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(\">>> 正在初始化环境...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)\n",
    "\n",
    "print(f\"Loading Base Model...\")\n",
    "model_base = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID, device_map=device, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")\n",
    "model_base.eval()\n",
    "\n",
    "print(f\"Loading Distilled Model...\")\n",
    "model_distilled = AutoModelForCausalLM.from_pretrained(\n",
    "    DISTILLED_MODEL_PATH, device_map=device, torch_dtype=torch.float16, low_cpu_mem_usage=True\n",
    ")\n",
    "model_distilled.eval()\n",
    "\n",
    "# ================= 改进后的推理函数 =================\n",
    "def get_response(model, question, force_thinking=False):\n",
    "    \"\"\"\n",
    "    force_thinking: 如果为 True，强制给模型“喂”一个开头，逼它进入思考模式\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # 1. 生成基础 Prompt\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # 2. 【核心技巧】Prompt 注入\n",
    "    # 如果是蒸馏模型，我们要检查它是否包含标签。\n",
    "    # 这里我们手动把 [思考过程] 拼在 Assistant 的开头\n",
    "    if force_thinking:\n",
    "        text += \"[思考过程]\\n\"\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False, \n",
    "            temperature=None,\n",
    "            top_p=None,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "        \n",
    "    # 解码\n",
    "    # 注意：如果使用了注入，生成的 tokens 里不包含注入的那部分，我们需要手动拼回去显示\n",
    "    input_len = inputs.input_ids.shape[1]\n",
    "    generated_ids = outputs[0][input_len:]\n",
    "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    if force_thinking:\n",
    "        \n",
    "        return \"thinking[思考过程]\\n\" + response\n",
    "    else:\n",
    "        return response\n",
    "\n",
    "# ================= 对比测试 =================\n",
    "\n",
    "questions = [\n",
    "    \"计算：24.6 ÷ 0.6\",\n",
    "    \"小明有 5 个苹果，小红的苹果数量是小明的 3 倍少 2 个。请问小红有多少个苹果？\",\n",
    "    \"如果 3x + 5 = 20，那么 x 等于多少？\"\n",
    "]\n",
    "\n",
    "separator = \"=\" * 80\n",
    "\n",
    "for i, q in enumerate(questions):\n",
    "    print(f\"\\n📝 测试题目 {i+1}: {q}\")\n",
    "    print(separator)\n",
    "    \n",
    "    # --- 原始模型 (不强制) ---\n",
    "    print(\"🔴 [原始 0.5B 模型] 回答：\")\n",
    "    try:\n",
    "        # 原始模型没学过这个格式，强制加反而会乱，所以不加\n",
    "        ans_base = get_response(model_base, q, force_thinking=False)\n",
    "        print(ans_base.strip())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # --- 蒸馏模型 (强制注入标签) ---\n",
    "    print(\"🟢 [蒸馏后 0.5B 模型] (触发思考模式)：\")\n",
    "    try:\n",
    "        # 我们来看看，一旦开了个头，它能不能接下去！\n",
    "        ans_distilled = get_response(model_distilled, q, force_thinking=True)\n",
    "        print(ans_distilled.strip())\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "    print(separator)\n",
    "    time.sleep(0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
