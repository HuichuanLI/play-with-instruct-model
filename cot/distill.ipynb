{
 "cells": [
  {
   "cell_type": "code",
   "id": "783eb2f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T16:29:06.830791Z",
     "start_time": "2025-12-28T16:28:28.723831Z"
    }
   },
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "# ================= é…ç½®åŒºåŸŸ =================\n",
    "# 1. æŒ‡å‘æœ¬åœ° Ollama æ¥å£\n",
    "OLLAMA_BASE_URL = \"http://192.168.1.10:11434/api/generate\"\n",
    "OLLAMA_MODEL_NAME = \"qwen3:latest\"  # ç¡®ä¿ä½ çš„Ollamaä¸­å­˜åœ¨è¿™ä¸ªæ¨¡å‹\n",
    "\n",
    "# ===========================================\n",
    "\n",
    "def distill_knowledge_local(question):\n",
    "    # é’ˆå¯¹ Qwen ä¼˜åŒ–çš„ Promptï¼Œå®ƒå¯¹ä¸­æ–‡æŒ‡ä»¤è·Ÿéšå¾ˆå¥½\n",
    "    prompt = f\"\"\"\n",
    "    ä½ æ˜¯ä¸€ä½é€»è¾‘ä¸¥å¯†çš„è€å¸ˆã€‚è¯·é’ˆå¯¹ä»¥ä¸‹é—®é¢˜ï¼Œç”Ÿæˆç”¨äºæ•™å­¦çš„æ€ç»´é“¾ï¼ˆChain of Thoughtï¼‰ã€‚\n",
    "    \n",
    "    é—®é¢˜ï¼š{question}\n",
    "    \n",
    "    è¯·åŠ¡å¿…ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼ç›´æ¥è¿”å›ï¼Œä¸è¦åŒ…å«Markdownä»£ç å—æ ‡è®°ï¼ˆå¦‚ ```json ï¼‰ï¼š\n",
    "    {{\n",
    "        \"instruction\": \"{question}\",\n",
    "        \"rationale\": \"è¿™é‡Œå†™ä¸‹è¯¦ç»†çš„ã€åˆ†æ­¥éª¤çš„æ¨ç†è¿‡ç¨‹ï¼Œå…ˆåˆ†æå†è®¡ç®—ï¼Œé€»è¾‘è¦æ¸…æ™°ã€‚\",\n",
    "        \"output\": \"æœ€ç»ˆçš„ç®€çŸ­ç­”æ¡ˆ\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # æ„é€ Ollama APIè¯·æ±‚å‚æ•°\n",
    "        payload = {\n",
    "            \"model\": OLLAMA_MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"format\": \"json\"  # å‘Šè¯‰Ollamaè¿”å›JSONæ ¼å¼å“åº”\n",
    "        }\n",
    "        \n",
    "        # å‘é€POSTè¯·æ±‚åˆ°Ollama API\n",
    "        response = requests.post(OLLAMA_BASE_URL, json=payload, timeout=30)\n",
    "        response.raise_for_status()  # æ£€æŸ¥HTTPé”™è¯¯\n",
    "        \n",
    "        # è§£æå“åº”å†…å®¹\n",
    "        content = response.json()\n",
    "        \n",
    "        if isinstance(content, dict) and \"response\" in content:\n",
    "            clean_content = content[\"response\"].replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "            try:\n",
    "                data_json = json.loads(clean_content)\n",
    "                return data_json\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"âŒ JSONè§£æå¤±è´¥ï¼Œæ¨¡å‹è¾“å‡ºäº†éæ ‡å‡†æ ¼å¼:\\n{clean_content}\\n\")\n",
    "                return None\n",
    "        else:\n",
    "            print(f\"âŒ å“åº”æ ¼å¼å¼‚å¸¸: {content}\")\n",
    "            return None\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"ç½‘ç»œè¯·æ±‚å‡ºé”™: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"APIè°ƒç”¨å‡ºé”™: {e}\")\n",
    "        return None\n",
    "\n",
    "# æµ‹è¯•è¿è¡Œï¼šæˆ‘ä»¬ç”¨å‡ ä¸ªæ•°å­¦é€»è¾‘é¢˜æ¥æµ‹è¯• Qwen çš„æ¨ç†ç”Ÿæˆèƒ½åŠ›\n",
    "seed_questions = [\n",
    "    \"å¦‚æœä½ æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰äº†1ä¸ªï¼Œåˆä¹°æ¥äº†5ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ\",\n",
    "    \"ä¸€ä¸ªç¬¼å­é‡Œæœ‰é¸¡å’Œå…”ï¼Œå¤´æœ‰10ä¸ªï¼Œè„šæœ‰28åªï¼Œé¸¡å…”å„å¤šå°‘ï¼Ÿ\",\n",
    "    \"ç”¨Pythonå†™ä¸€ä¸ªå‡½æ•°ï¼Œåˆ¤æ–­ä¸€ä¸ªæ•°å­—æ˜¯ä¸æ˜¯ç´ æ•°ã€‚\"\n",
    "]\n",
    "\n",
    "dataset = []\n",
    "\n",
    "print(f\"æ­£åœ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹ {OLLAMA_MODEL_NAME} è¿›è¡Œè’¸é¦...\\n\")\n",
    "\n",
    "for q in seed_questions:\n",
    "    print(f\"æ­£åœ¨å¤„ç†: {q} . ..\")\n",
    "    content = distill_knowledge_local(q)\n",
    "    \n",
    "    if content:\n",
    "        # æ¸…æ´—æ•°æ®ï¼šæœ‰æ—¶å€™æ¨¡å‹ä¼šå¿ä¸ä½åŠ  ```json ... ```ï¼Œæˆ‘ä»¬éœ€è¦å»æ‰\n",
    "        try:\n",
    "            data_json = content\n",
    "            dataset.append(data_json)\n",
    "            print(\"âœ… ç”ŸæˆæˆåŠŸï¼\")\n",
    "            print(f\"  --> æ€ç»´è¿‡ç¨‹ç‰‡æ®µ: {data_json['rationale'][:50]}. ..\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æ•°æ®å¤„ç†å¼‚å¸¸: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ ç”Ÿæˆå¤±è´¥ï¼Œæ— è¿”å›å†…å®¹ã€‚\")\n",
    "\n",
    "# ä¿å­˜ç»“æœ\n",
    "if dataset:\n",
    "    filename = \"local_distill_data.jsonl\"\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in dataset:\n",
    "            final_entry = {\n",
    "                \"instruction\": entry[\"instruction\"],\n",
    "                \"input\": \"\",\n",
    "                \"output\": f\"ã€æ€è€ƒè¿‡ç¨‹ã€‘\\n{entry['rationale']}\\n\\nã€æœ€ç»ˆç­”æ¡ˆã€‘\\n{entry['output']}\"\n",
    "            }\n",
    "            json.dump(final_entry, f, ensure_ascii=False)\n",
    "            f.write(\"\\n\")\n",
    "    print(f\"\\nğŸ‰ ä»»åŠ¡å®Œæˆï¼æ•°æ®å·²ä¿å­˜ä¸º {filename}ï¼Œå…± {len(dataset)} æ¡ã€‚\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä½¿ç”¨æœ¬åœ°æ¨¡å‹ qwen3:latest è¿›è¡Œè’¸é¦...\n",
      "\n",
      "æ­£åœ¨å¤„ç†: å¦‚æœä½ æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰äº†1ä¸ªï¼Œåˆä¹°æ¥äº†5ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ . ..\n",
      "ç½‘ç»œè¯·æ±‚å‡ºé”™: HTTPConnectionPool(host='192.168.1.10', port=11434): Max retries exceeded with url: /api/generate (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x10f72ed30>, 'Connection to 192.168.1.10 timed out. (connect timeout=30)'))\n",
      "âŒ ç”Ÿæˆå¤±è´¥ï¼Œæ— è¿”å›å†…å®¹ã€‚\n",
      "æ­£åœ¨å¤„ç†: ä¸€ä¸ªç¬¼å­é‡Œæœ‰é¸¡å’Œå…”ï¼Œå¤´æœ‰10ä¸ªï¼Œè„šæœ‰28åªï¼Œé¸¡å…”å„å¤šå°‘ï¼Ÿ . ..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 73\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m q \u001B[38;5;129;01min\u001B[39;00m seed_questions:\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mæ­£åœ¨å¤„ç†: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mq\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m . ..\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 73\u001B[0m     content \u001B[38;5;241m=\u001B[39m \u001B[43mdistill_knowledge_local\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m content:\n\u001B[1;32m     76\u001B[0m         \u001B[38;5;66;03m# æ¸…æ´—æ•°æ®ï¼šæœ‰æ—¶å€™æ¨¡å‹ä¼šå¿ä¸ä½åŠ  ```json ... ```ï¼Œæˆ‘ä»¬éœ€è¦å»æ‰\u001B[39;00m\n\u001B[1;32m     77\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "Cell \u001B[0;32mIn[1], line 36\u001B[0m, in \u001B[0;36mdistill_knowledge_local\u001B[0;34m(question)\u001B[0m\n\u001B[1;32m     28\u001B[0m payload \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m: OLLAMA_MODEL_NAME,\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m\"\u001B[39m: prompt,\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjson\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# å‘Šè¯‰Ollamaè¿”å›JSONæ ¼å¼å“åº”\u001B[39;00m\n\u001B[1;32m     33\u001B[0m }\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# å‘é€POSTè¯·æ±‚åˆ°Ollama API\u001B[39;00m\n\u001B[0;32m---> 36\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\u001B[43mOLLAMA_BASE_URL\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpayload\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m response\u001B[38;5;241m.\u001B[39mraise_for_status()  \u001B[38;5;66;03m# æ£€æŸ¥HTTPé”™è¯¯\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# è§£æå“åº”å†…å®¹\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/requests/api.py:115\u001B[0m, in \u001B[0;36mpost\u001B[0;34m(url, data, json, **kwargs)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(url, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, json\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    104\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a POST request.\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \n\u001B[1;32m    106\u001B[0m \u001B[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 115\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpost\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjson\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/requests/api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[0;34m(method, url, **kwargs)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[0;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/requests/sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[1;32m    584\u001B[0m send_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    585\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m: timeout,\n\u001B[1;32m    586\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mallow_redirects\u001B[39m\u001B[38;5;124m\"\u001B[39m: allow_redirects,\n\u001B[1;32m    587\u001B[0m }\n\u001B[1;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[0;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/requests/sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[0;34m(self, request, **kwargs)\u001B[0m\n\u001B[1;32m    700\u001B[0m start \u001B[38;5;241m=\u001B[39m preferred_clock()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[0;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m \u001B[43madapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n\u001B[1;32m    706\u001B[0m elapsed \u001B[38;5;241m=\u001B[39m preferred_clock() \u001B[38;5;241m-\u001B[39m start\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/requests/adapters.py:667\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[1;32m    664\u001B[0m     timeout \u001B[38;5;241m=\u001B[39m TimeoutSauce(connect\u001B[38;5;241m=\u001B[39mtimeout, read\u001B[38;5;241m=\u001B[39mtimeout)\n\u001B[1;32m    666\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 667\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    668\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    669\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    670\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    671\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    672\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    673\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    674\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    675\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    676\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    677\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    678\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    679\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    681\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    682\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(err, request\u001B[38;5;241m=\u001B[39mrequest)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/urllib3/connectionpool.py:789\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001B[0m\n\u001B[1;32m    786\u001B[0m response_conn \u001B[38;5;241m=\u001B[39m conn \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m release_conn \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    788\u001B[0m \u001B[38;5;66;03m# Make the request on the HTTPConnection object\u001B[39;00m\n\u001B[0;32m--> 789\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    790\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    791\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    792\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    793\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    794\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43mresponse_conn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_conn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    801\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mresponse_kw\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    802\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;66;03m# Everything went great!\u001B[39;00m\n\u001B[1;32m    805\u001B[0m clean_exit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/urllib3/connectionpool.py:495\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    492\u001B[0m \u001B[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001B[39;00m\n\u001B[1;32m    493\u001B[0m \u001B[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001B[39;00m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 495\u001B[0m     \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreload_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdecode_content\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43menforce_content_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menforce_content_length\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    506\u001B[0m \u001B[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001B[39;00m\n\u001B[1;32m    507\u001B[0m \u001B[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001B[39;00m\n\u001B[1;32m    508\u001B[0m \u001B[38;5;66;03m# With this behaviour, the received response is still readable.\u001B[39;00m\n\u001B[1;32m    509\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBrokenPipeError\u001B[39;00m:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/urllib3/connection.py:441\u001B[0m, in \u001B[0;36mHTTPConnection.request\u001B[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m header, value \u001B[38;5;129;01min\u001B[39;00m headers\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mputheader(header, value)\n\u001B[0;32m--> 441\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mendheaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    443\u001B[0m \u001B[38;5;66;03m# If we're given a body we start sending that in chunks.\u001B[39;00m\n\u001B[1;32m    444\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunks \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/http/client.py:1280\u001B[0m, in \u001B[0;36mHTTPConnection.endheaders\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1278\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1279\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CannotSendHeader()\n\u001B[0;32m-> 1280\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmessage_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencode_chunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencode_chunked\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/http/client.py:1040\u001B[0m, in \u001B[0;36mHTTPConnection._send_output\u001B[0;34m(self, message_body, encode_chunked)\u001B[0m\n\u001B[1;32m   1038\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\r\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer)\n\u001B[1;32m   1039\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_buffer[:]\n\u001B[0;32m-> 1040\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmsg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1042\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m message_body \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1043\u001B[0m \n\u001B[1;32m   1044\u001B[0m     \u001B[38;5;66;03m# create a consistent interface to message_body\u001B[39;00m\n\u001B[1;32m   1045\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(message_body, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mread\u001B[39m\u001B[38;5;124m'\u001B[39m):\n\u001B[1;32m   1046\u001B[0m         \u001B[38;5;66;03m# Let file-like take precedence over byte-like.  This\u001B[39;00m\n\u001B[1;32m   1047\u001B[0m         \u001B[38;5;66;03m# is needed to allow the current position of mmap'ed\u001B[39;00m\n\u001B[1;32m   1048\u001B[0m         \u001B[38;5;66;03m# files to be taken into account.\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/http/client.py:980\u001B[0m, in \u001B[0;36mHTTPConnection.send\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m    978\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    979\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_open:\n\u001B[0;32m--> 980\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    981\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    982\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m NotConnected()\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/urllib3/connection.py:279\u001B[0m, in \u001B[0;36mHTTPConnection.connect\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    278\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 279\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msock \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    280\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tunnel_host:\n\u001B[1;32m    281\u001B[0m         \u001B[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001B[39;00m\n\u001B[1;32m    282\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_connected_to_proxy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/urllib3/connection.py:199\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001B[39;00m\n\u001B[1;32m    195\u001B[0m \n\u001B[1;32m    196\u001B[0m \u001B[38;5;124;03m:return: New socket connection.\u001B[39;00m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    198\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 199\u001B[0m     sock \u001B[38;5;241m=\u001B[39m \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_connection\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    200\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dns_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mport\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    201\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43msource_address\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msource_address\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43msocket_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msocket_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    204\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m socket\u001B[38;5;241m.\u001B[39mgaierror \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NameResolutionError(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost, \u001B[38;5;28mself\u001B[39m, e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/urllib3/util/connection.py:73\u001B[0m, in \u001B[0;36mcreate_connection\u001B[0;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m source_address:\n\u001B[1;32m     72\u001B[0m     sock\u001B[38;5;241m.\u001B[39mbind(source_address)\n\u001B[0;32m---> 73\u001B[0m \u001B[43msock\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43msa\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;66;03m# Break explicitly a reference cycle\u001B[39;00m\n\u001B[1;32m     75\u001B[0m err \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "87fb83cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T16:29:16.803561Z",
     "start_time": "2025-12-28T16:29:09.563905Z"
    }
   },
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "\n",
    "# 1. åŠ è½½å­¦ç”Ÿæ¨¡å‹çš„ Tokenizer (å‡è®¾ä½ ç”¨çš„æ˜¯ Qwen2.5-0.5B)\n",
    "# å¦‚æœæœ¬åœ°è¿˜æ²¡ä¸‹è½½ï¼Œå®ƒä¼šè‡ªåŠ¨ä¸‹è½½\n",
    "model_id = \"Qwen/Qwen2.5-0.5B-Instruct\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# ç¡®ä¿ pad_token å­˜åœ¨\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def process_func(example):\n",
    "    \"\"\"\n",
    "    å°†ç”Ÿæˆçš„ CoT æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ ¼å¼\n",
    "    Example æ ¼å¼: {\"instruction\": \"...\", \"input\": \"\", \"output\": \"ã€æ€è€ƒè¿‡ç¨‹ã€‘...\"}\n",
    "    \"\"\"\n",
    "    MAX_LENGTH = 512 # æ¼”ç¤ºç”¨ï¼Œå®é™…å¯è°ƒå¤§\n",
    "    \n",
    "    instruction = example[\"instruction\"]\n",
    "    output = example[\"output\"] # è¿™é‡ŒåŒ…å«äº† Teacher ç”Ÿæˆçš„æ€ç»´é“¾\n",
    "\n",
    "    # Qwen çš„æ ‡å‡†å¯¹è¯æ¨¡æ¿æ„å»º\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "        {\"role\": \"assistant\", \"content\": output}\n",
    "    ]\n",
    "    \n",
    "    # ä½¿ç”¨ apply_chat_template è‡ªåŠ¨å¤„ç† <|im_start|> ç­‰ç‰¹æ®Š token\n",
    "    # è¿™ä¸€æ­¥éå¸¸å…³é”®ï¼Œä¸åŒæ¨¡å‹æ¨¡æ¿ä¸åŒ\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    # è½¬æ¢ä¸º ID\n",
    "    input_ids = tokenizer(text + tokenizer.eos_token, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    \n",
    "    # === å…³é”®ç‚¹ï¼šåˆ¶ä½œ Labels (Masking) ===\n",
    "    # æˆ‘ä»¬ä¸å¸Œæœ›æ¨¡å‹å­¦ä¹  \"User\" è¯´çš„è¯ï¼Œåªå­¦ä¹  \"Assistant\" è¯´çš„è¯\n",
    "    # æ‰€ä»¥è¦æŠŠ User éƒ¨åˆ†çš„ token åœ¨ label ä¸­è®¾ä¸º -100 (PyTorch ä¼šå¿½ç•¥)\n",
    "    \n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # æ‰¾åˆ° \"assistant\" å›å¤å¼€å§‹çš„ä½ç½®\n",
    "    # Qwen çš„æ¨¡æ¿é€šå¸¸æ˜¯: ... <|im_start|>assistant\\n\n",
    "    # æˆ‘ä»¬ç®€å•ç²—æš´åœ°é‡æ–° tokenize \"user\" éƒ¨åˆ†æ¥æ‰¾é•¿åº¦ï¼ˆä¸¥è°¨çš„åšæ³•éœ€æ›´å¤æ‚åŒ¹é…ï¼‰\n",
    "    \n",
    "    user_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction}\n",
    "    ]\n",
    "    user_text = tokenizer.apply_chat_template(user_messages, tokenize=False, add_generation_prompt=True)\n",
    "    user_ids = tokenizer(user_text, return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "    \n",
    "    user_len = len(user_ids)\n",
    "    \n",
    "    # å°† User éƒ¨åˆ†çš„ Label è®¾ä¸º -100\n",
    "    labels[:user_len] = -100\n",
    "    \n",
    "    # æˆªæ–­æˆ–å¡«å……\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "        \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"attention_mask\": torch.ones_like(input_ids) # ç®€å•å¤„ç†\n",
    "    }\n",
    "\n",
    "# === æµ‹è¯•è¿è¡Œ ===\n",
    "# è¯»å–æˆ‘ä»¬åˆšæ‰ç”Ÿæˆçš„ local_distill_data.jsonl çš„ç¬¬ä¸€æ¡\n",
    "with open(\"local_distill_data.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "\n",
    "processed = process_func(first_line)\n",
    "\n",
    "print(f\"åŸå§‹é—®é¢˜: {first_line['instruction']}\")\n",
    "print(f\"Token æ€»é•¿åº¦: {len(processed['input_ids'])}\")\n",
    "print(f\"Labels å‰10ä¸ª (åº”è¯¥æ˜¯-100): {processed['labels'][:10]}\")\n",
    "print(f\"Labels æœ€å10ä¸ª (åº”è¯¥æœ‰å€¼): {processed['labels'][-10:]}\")\n",
    "\n",
    "# è§£ç çœ‹çœ‹æ¨¡å‹åˆ°åº•è¦åœ¨å“ªä¸€éƒ¨åˆ†è®¡ç®— Loss\n",
    "valid_labels = processed['labels'][processed['labels'] != -100]\n",
    "print(\"\\n=== æ¨¡å‹å°†å­¦ä¹ çš„å†…å®¹ (Teacher çš„ CoT) ===\")\n",
    "print(tokenizer.decode(valid_labels))"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'local_distill_data.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 77\u001B[0m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[1;32m     70\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minput_ids\u001B[39m\u001B[38;5;124m\"\u001B[39m: input_ids,\n\u001B[1;32m     71\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m: labels,\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattention_mask\u001B[39m\u001B[38;5;124m\"\u001B[39m: torch\u001B[38;5;241m.\u001B[39mones_like(input_ids) \u001B[38;5;66;03m# ç®€å•å¤„ç†\u001B[39;00m\n\u001B[1;32m     73\u001B[0m     }\n\u001B[1;32m     75\u001B[0m \u001B[38;5;66;03m# === æµ‹è¯•è¿è¡Œ ===\u001B[39;00m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;66;03m# è¯»å–æˆ‘ä»¬åˆšæ‰ç”Ÿæˆçš„ local_distill_data.jsonl çš„ç¬¬ä¸€æ¡\u001B[39;00m\n\u001B[0;32m---> 77\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlocal_distill_data.jsonl\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mutf-8\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     78\u001B[0m     first_line \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(f\u001B[38;5;241m.\u001B[39mreadline())\n\u001B[1;32m     80\u001B[0m processed \u001B[38;5;241m=\u001B[39m process_func(first_line)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/llma3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001B[0m, in \u001B[0;36m_modified_open\u001B[0;34m(file, *args, **kwargs)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m file \u001B[38;5;129;01min\u001B[39;00m {\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m}:\n\u001B[1;32m    304\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    305\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIPython won\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt let you open fd=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m by default \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myou can use builtins\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m open.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    308\u001B[0m     )\n\u001B[0;32m--> 310\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mio_open\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'local_distill_data.jsonl'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "72e7ec43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T16:29:16.805237Z",
     "start_time": "2025-12-28T16:29:16.805104Z"
    }
   },
   "source": [
    "!pip install datasets\n",
    "!pip install trl\n",
    "!pip install tf-keras"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0cbd20e8",
   "metadata": {},
   "source": [
    "import transformers\n",
    "import trl\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(trl.__version__)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. å¼•å…¥ PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶\n",
    "# ä½œç”¨ï¼šè¿™æ˜¯åº•å±‚åŸºç¡€æ¡†æ¶ï¼Œç”¨äºå¼ é‡è®¡ç®—ã€è‡ªåŠ¨æ±‚å¯¼ä»¥åŠç®¡ç† GPU/CPU èµ„æºã€‚\n",
    "import torch\n",
    "# 2. ä» datasets åº“ä¸­å¼•å…¥ load_dataset å‡½æ•°\n",
    "# ä½œç”¨ï¼šç”¨äºåŠ è½½è®­ç»ƒæ•°æ®ã€‚æ”¯æŒä» Hugging Face Hub åœ¨çº¿åŠ è½½ï¼Œä¹Ÿæ”¯æŒåŠ è½½æœ¬åœ°çš„ JSON/CSV/Parquet æ–‡ä»¶ã€‚\n",
    "from datasets import load_dataset\n",
    "# 3. ä» transformers åº“ä¸­å¼•å…¥ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶\n",
    "# AutoTokenizer: è‡ªåŠ¨åŠ è½½åˆ†è¯å™¨ï¼Œå°†æ–‡æœ¬è½¬æ¢æˆæ¨¡å‹èƒ½è¯»æ‡‚çš„æ•°å­—ï¼ˆToken IDï¼‰ã€‚\n",
    "# AutoModelForCausalLM: è‡ªåŠ¨åŠ è½½å› æœè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ Llama, Qwen, GPT ç­‰è‡ªå›å½’æ¨¡å‹ï¼‰çš„ç»“æ„å’Œæƒé‡ã€‚\n",
    "# TrainingArguments: ç”¨äºå®šä¹‰è®­ç»ƒæ—¶çš„è¶…å‚æ•°ï¼ˆå¦‚å­¦ä¹ ç‡ã€Batch Sizeã€Epochæ•°ã€ä¿å­˜è·¯å¾„ç­‰ï¼‰ã€‚\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "# 4. ä» trl (Transformer Reinforcement Learning) åº“ä¸­å¼•å…¥ SFT è®­ç»ƒå™¨å’Œæ•°æ®æ•´ç†å™¨\n",
    "# SFTTrainer: ä¸“é—¨ç”¨äºç›‘ç£å¾®è°ƒï¼ˆSupervised Fine-Tuningï¼‰çš„è®­ç»ƒå™¨ï¼Œç®€åŒ–äº†æµç¨‹ã€‚\n",
    "# DataCollatorForCompletionOnlyLM: ä¸€ç§ç‰¹æ®Šçš„æ•°æ®æ•´ç†å™¨ã€‚åœ¨æŒ‡ä»¤å¾®è°ƒä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æ¨¡å‹åªå­¦ä¹ â€œå›ç­”â€éƒ¨åˆ†ï¼Œ\n",
    "# è€Œä¸è®¡ç®—â€œæé—®â€éƒ¨åˆ†çš„ Lossã€‚è¿™ä¸ªå·¥å…·å¯ä»¥è‡ªåŠ¨æŠŠâ€œæé—®â€éƒ¨åˆ†æ©ç›–ï¼ˆMaskï¼‰æ‰ã€‚\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "# 5. ä» peft åº“ä¸­å¼•å…¥ LoraConfig\n",
    "# ä½œç”¨ï¼šPEFT (Parameter-Efficient Fine-Tuning) æ˜¯å‚æ•°é«˜æ•ˆå¾®è°ƒåº“ã€‚\n",
    "# LoraConfig ç”¨äºé…ç½® LoRA (Low-Rank Adaptation) ç®—æ³•çš„å‚æ•°ï¼ˆå¦‚ç§© rank, alpha ç­‰ï¼‰ï¼Œ\n",
    "# è®©æˆ‘ä»¬ä¸éœ€è¦å…¨é‡å¾®è°ƒæ¨¡å‹ï¼Œåªéœ€å¾®è°ƒæå°‘é‡çš„å‚æ•°ï¼Œå¤§å¤§é™ä½æ˜¾å­˜éœ€æ±‚ã€‚\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147cb708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e2665b7dfa45d6acf8b9b047f6de72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x21e4c460eb0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 0b0e7eb8-907e-4eff-a647-131488dccde9)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<HTTPSConnection(host='huggingface.co', port=443) at 0x21e4c461870>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: de853889-6e2e-4a3c-ad27-13ed07cce1ad)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b05fd37d59d846659ab85036e2de3a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd89817db54431e869dfb77ec2f121d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3a482ca474437bada2b3be10ee924c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# ================= é…ç½® =================\n",
    "MODEL_ID = \"Qwen/Qwen2.5-0.5B-Instruct\" \n",
    "DATA_FILE = \"math_dataset_with_answers.jsonl\" \n",
    "\n",
    "# ç”¨äºæŒ‡å®šè®­ç»ƒå®Œæˆåæ¨¡å‹ã€æ—¥å¿—å’Œæ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰ä¿å­˜çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚\n",
    "# åç»­åœ¨å®šä¹‰ TrainingArguments æ—¶ä¼šç”¨åˆ°è¿™ä¸ªå˜é‡ã€‚å¦‚æœè¯¥æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œè®­ç»ƒè„šæœ¬é€šå¸¸ä¼šè‡ªåŠ¨åˆ›å»ºå®ƒã€‚\n",
    "OUTPUT_DIR = \"./qwen2.5_distilled_checkpoint\"\n",
    "\n",
    "# 1. åŠ è½½æ•°æ®\n",
    "# ä½¿ç”¨ HuggingFace dataset åº“ç›´æ¥è¯»å– jsonl\n",
    "\n",
    "# json, æŒ‡å®šæ•°æ®çš„è§£ææ ¼å¼ã€‚è¿™æ„å‘³ç€ datasets åº“ä¼šä½¿ç”¨å†…ç½®çš„ JSON åŠ è½½è„šæœ¬å»è¯»å–æ–‡ä»¶ã€‚é™¤äº† \"json\"ï¼Œå¸¸ç”¨çš„è¿˜æœ‰ \"csv\", \"parquet\", \"text\" ç­‰ã€‚\n",
    "# data_files, è¿™ä¸ªå‚æ•°å‘Šè¯‰å‡½æ•°å»å“ªé‡Œæ‰¾æ•°æ®ã€‚æ”¯æŒä¼ å…¥å•ä¸ªæ–‡ä»¶è·¯å¾„ï¼Œä¹Ÿæ”¯æŒä¼ å…¥æ–‡ä»¶åˆ—è¡¨ã€‚\n",
    "# split, æŒ‡å®šåŠ è½½æ•°æ®åçš„åˆ’åˆ†æ ‡ç­¾ã€‚load_dataset é»˜è®¤ä¼šè¿”å›ä¸€ä¸ª DatasetDictï¼ˆåŒ…å« train/test/valid ç­‰ï¼‰ã€‚\n",
    "# è®¾ç½® split=\"train\" åï¼Œå®ƒä¼šç›´æ¥è¿”å›ä¸€ä¸ªå•çº¯çš„ Dataset å¯¹è±¡ï¼Œé‡Œé¢åŒ…å«å…¨é‡æ•°æ®ã€‚\n",
    "# å¦‚æœä¸åŠ è¿™ä¸€è¡Œï¼Œä½ åç»­è°ƒç”¨æ•°æ®æ—¶éœ€è¦ç”¨ dataset[\"train\"] æ‰èƒ½å–åˆ°æ•°æ®ã€‚\n",
    "dataset = load_dataset(\"json\", data_files=DATA_FILE, split=\"train\")\n",
    "\n",
    "# 2. åŠ è½½æ¨¡å‹ä¸Tokenizer\n",
    "# æ ¹æ®æ¨¡å‹ ID åŠ è½½å¯¹åº”çš„åˆ†è¯å™¨ã€‚åˆ†è¯å™¨è´Ÿè´£å°†æ–‡æœ¬åˆ‡åˆ†å¹¶è½¬æ¢ä¸ºæ•°å­— IDã€‚\n",
    "\n",
    "# MODEL_ID, æ¨¡å‹åœ¨ Hugging Face Hub ä¸Šçš„ IDï¼ˆå¦‚ \"Qwen/Qwen2.5-7B\"ï¼‰æˆ–è€…æœ¬åœ°æ¨¡å‹çš„å­˜å‚¨è·¯å¾„ã€‚\n",
    "# trust_remote_code=True, éå¸¸é‡è¦ã€‚è®¸å¤šæ–°æ¨¡å‹ï¼ˆå¦‚ Qwen, ChatGLM ç­‰ï¼‰çš„åˆ†è¯é€»è¾‘å¹¶æ²¡æœ‰é›†æˆåœ¨å®˜æ–¹çš„ transformers åº“ä¸­ï¼Œ\n",
    "# è€Œæ˜¯æ”¾åœ¨äº†æ¨¡å‹ä»“åº“é‡Œçš„ Python æ–‡ä»¶ï¼ˆå¦‚ tokenization_qwen.pyï¼‰ä¸­ã€‚\n",
    "# è®¾ç½®ä¸º True, è¡¨ç¤ºä½ å…è®¸ä»è¯¥æ¨¡å‹ä»“åº“ä¸‹è½½å¹¶æ‰§è¡Œè¿™äº›è‡ªå®šä¹‰çš„ Python ä»£ç ã€‚å¦‚æœä¸è®¾ç½®ï¼ŒåŠ è½½æ–°æ¨¡å‹é€šå¸¸ä¼šæŠ¥é”™ã€‚\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "# æ£€æŸ¥åˆ†è¯å™¨æ˜¯å¦æœ‰â€œå¡«å……ç¬¦â€ï¼ˆpad_tokenï¼‰ï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°±ç”¨â€œç»“æŸç¬¦â€ï¼ˆeos_tokenï¼‰ä»£æ›¿ã€‚\n",
    "# åœ¨æ‰¹é‡è®­ç»ƒï¼ˆBatch Trainingï¼‰æ—¶ï¼Œå¿…é¡»æŠŠé•¿çŸ­ä¸ä¸€çš„å¥å­è¡¥é½æˆåŒæ ·çš„é•¿åº¦ï¼ˆçŸ©é˜µè¦æ±‚ï¼‰ï¼Œè¿™å°±éœ€è¦ç”¨åˆ° pad_tokenã€‚\n",
    "# ç°åœ¨ï¼Œå¾ˆå¤šç°ä»£å¤§æ¨¡å‹ï¼ˆå¦‚ Llama 2, Llama 3, Qwen ç­‰ï¼‰ä¸ºäº†èŠ‚çœè¯è¡¨ç©ºé—´ï¼Œé»˜è®¤æ²¡æœ‰å®šä¹‰ pad_tokenã€‚\n",
    "# å¦‚æœä¸æ‰‹åŠ¨æŒ‡å®šï¼Œè®­ç»ƒæ—¶ä¼šæŠ¥é”™ã€‚å°† eos_tokenï¼ˆEnd of Sentenceï¼‰ä½œä¸ºå¡«å……ç¬¦æ˜¯ä¸€ç§é€šç”¨çš„è§£å†³æ–¹æ¡ˆã€‚\n",
    "if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# åŠ è½½æ¨¡å‹ (è¿™é‡Œä¸ºäº†æ¼”ç¤ºç®€å•ï¼Œæ²¡æœ‰ç”¨é‡åŒ–ï¼Œ0.5B å¾ˆå°ï¼Œæ˜¾å­˜å¤Ÿç”¨)\n",
    "# MODEL_IDï¼ŒæŒ‡å®šè¦åŠ è½½çš„æ¨¡å‹è·¯å¾„æˆ– IDã€‚\n",
    "# device_map=\"auto\"ï¼Œæ™ºèƒ½æ˜¾å­˜åˆ†é…ã€‚\n",
    "#   è¿™ä¸ªå‚æ•°ä¼šè®© accelerate åº“è‡ªåŠ¨è®¡ç®—å¦‚ä½•åŠ è½½æ¨¡å‹ã€‚\n",
    "#   å¦‚æœæ˜¯ä¸€å¼ å¡ï¼Œå®ƒä¼šæŠŠæ¨¡å‹å…¨æ”¾è¿›å»ã€‚\n",
    "#   å¦‚æœæ˜¯å¤šå¼ å¡ï¼Œå®ƒä¼šè‡ªåŠ¨åˆ‡åˆ†æ¨¡å‹å±‚ï¼Œå‡åŒ€åˆ†é…åˆ°ä¸åŒ GPU ä¸Šã€‚\n",
    "#   å¦‚æœæ˜¾å­˜ä¸å¤Ÿï¼Œå®ƒç”šè‡³ä¼šå°†éƒ¨åˆ†å±‚å¸è½½ï¼ˆOffloadï¼‰åˆ° CPU å†…å­˜ç”šè‡³ç¡¬ç›˜ä¸Šã€‚\n",
    "# torch_dtype=torch.float16ï¼Œç²¾åº¦è®¾ç½®ã€‚\n",
    "#   é»˜è®¤æƒ…å†µï¼Œæ¨¡å‹æƒé‡é€šå¸¸æ˜¯ FP32ï¼ˆ32ä½æµ®ç‚¹æ•°ï¼‰ï¼Œå ç”¨æ˜¾å­˜å¤§ã€‚\n",
    "#   torch.float16 (FP16)ï¼ŒåŠç²¾åº¦æµ®ç‚¹æ•°ã€‚è¿™å¯ä»¥å°†æ˜¾å­˜å ç”¨ç›´æ¥å‡åŠï¼Œä¸”å¯¹æ¨ç†å’Œå¾®è°ƒçš„ç²¾åº¦å½±å“å¾ˆå°ã€‚\n",
    "#   æ³¨ï¼šå¦‚æœæ˜¯æ¯”è¾ƒæ–°çš„æ˜¾å¡ï¼ˆå¦‚ RTX 30/40ç³»åˆ—ï¼ŒA100/H100ï¼‰ï¼Œè¿™é‡Œæ¨èç”¨ torch.bfloat16ï¼Œå®ƒçš„æ•°å€¼ç¨³å®šæ€§æ¯” float16 æ›´å¥½ï¼Œè®­ç»ƒä¸å®¹æ˜“å‘æ•£ã€‚\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dcf0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å¼€å§‹é»‘ç›’è’¸é¦è®­ç»ƒ (SFT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a55be25e-c36e-41c7-a789-b7b183ddd5c8)')' thrown while requesting HEAD https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\peft\\mapping_func.py:72: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "d:\\ProgramData\\miniconda3\\envs\\yolov8\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7ec526b18c4c7a8eb67a8a14c3bc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace1de6af8064d6bb3c52c2206086028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb008143501433fa1de04a87faced11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding EOS to train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03255000e9b42f9a12bf3ae7dbd2eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5123ac4d7e264130a5c2d948c225858e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 01:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.808600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.612200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.538900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.473800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.579300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.513800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.440900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.423300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.390500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.409000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.317600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.398800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.341800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.401000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.366200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.390700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.382800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.360400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.364700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.331300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.375200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.275800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.307300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.262600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.219200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.268100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.355600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.221600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.263700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.250600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.261400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.239000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.238800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.235500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.287800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.255600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.268000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.188800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.201200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.212100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.197300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.193900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.150600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.177300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.211500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.230900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.166400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.268900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.186400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.163100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.168400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.150400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.215800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.218600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.157500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.198900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.163900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.161300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.197400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.152300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.157100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.165600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.154100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.162100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.160700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ è®­ç»ƒå®Œæˆï¼ä¿å­˜é€‚é…å™¨åˆ° ./qwen2.5_distilled_checkpoint\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. è®¾ç½® LoRA (å¯é€‰ï¼Œä½†æ¨è)\n",
    "# å…¨å‚æ•°å¾®è°ƒ0.5Bä¹Ÿå¯ä»¥ï¼Œä½†LoRAæ›´å¿«æ›´ç¨³\n",
    "# LoRAä¸æ›´æ–°æ¨¡å‹åŸæœ¬çš„å‡ åäº¿å‚æ•°ï¼Œè€Œæ˜¯ç»™æ¨¡å‹ä¸­æŒ‡å®šçš„å±‚æ’å…¥ä¸¤ä¸ªä½ç§©å°çŸ©é˜µæ¥è®­ç»ƒã€‚è¿™æ ·æ˜¾å­˜å ç”¨æä½ï¼Œä¸”æ•ˆæœé€¼è¿‘å…¨é‡å¾®è°ƒã€‚\n",
    "# r=8ï¼Œç§© (Rank)ã€‚\n",
    "#   è¿™æ˜¯ LoRA ä¸­æœ€é‡è¦çš„å‚æ•°ã€‚å®ƒå†³å®šäº†å¯è®­ç»ƒå‚æ•°çš„æ•°é‡ã€‚æ•°å€¼è¶Šå¤§ï¼Œæ¨¡å‹èƒ½å­¦åˆ°çš„ä¿¡æ¯è¶Šå¤šï¼Œä½†æ˜¾å­˜å ç”¨å’Œè®¡ç®—é‡ä¹Ÿè¶Šå¤§ã€‚\n",
    "#   é€šå¸¸è®¾ä¸º 8, 16, 32 æˆ– 64ã€‚å¯¹äºä¸€èˆ¬å¾®è°ƒï¼Œ8 æˆ– 16 æ˜¯æ€§ä»·æ¯”æœ€é«˜çš„é€‰æ‹©ã€‚\n",
    "# lora_alpha=16ï¼Œç¼©æ”¾ç³»æ•° (Scaling Factor)ã€‚\n",
    "#   ç±»ä¼¼äºå­¦ä¹ ç‡æ”¾å¤§å™¨ã€‚LoRA çš„æƒé‡æ›´æ–°ä¼šè¢«ä¹˜ä»¥ alpha/rã€‚\n",
    "#   é€šå¸¸è®¾ç½®ä¸º r çš„ 2å€ï¼ˆå³ alpha=2rï¼‰æ˜¯ç»éªŒä¸Šçš„æœ€ä½³å®è·µã€‚\n",
    "# target_modules=[...]ï¼Œç›®æ ‡æ¨¡å—ã€‚æŒ‡å®šè¦å¯¹æ¨¡å‹çš„å“ªäº›å±‚åº”ç”¨ LoRAã€‚\n",
    "#   åˆ—è¡¨ä¸­çš„åå­—ï¼ˆå¦‚ q_projï¼‰å¯¹åº” Transformer å†…éƒ¨çš„çº¿æ€§å±‚åç§°ã€‚\n",
    "#   ç‰¹åˆ«è¯´æ˜ï¼Œå¯¹äº Qwenã€Llama ç­‰æ¨¡å‹ï¼Œå®˜æ–¹å»ºè®®å¯¹æ‰€æœ‰çº¿æ€§å±‚ï¼ˆAttention å±‚çš„ Q,K,V,O å’Œ MLP å±‚çš„ Gate,Up,Downï¼‰éƒ½è¿›è¡Œå¾®è°ƒï¼Œæ•ˆæœè¿œå¥½äºåªå¾®è°ƒ Attention å±‚ã€‚\n",
    "# lora_dropout=0.05ï¼ŒDropout ç‡ã€‚\n",
    "#   åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒ 5% çš„ç¥ç»å…ƒè¿æ¥ï¼Œé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼ˆæ­»è®°ç¡¬èƒŒè®­ç»ƒæ•°æ®ï¼‰ã€‚\n",
    "# bias=\"none\"ï¼Œåç½®é¡¹è®¾ç½®ã€‚\n",
    "#   \"none\" è¡¨ç¤ºä¸è®­ç»ƒåŸæ¨¡å‹ä¸­çš„ Bias å‚æ•°ã€‚è¿™æ˜¯ä¸ºäº†æœ€å¤§ç¨‹åº¦èŠ‚çœæ˜¾å­˜ã€‚ä¹Ÿå¯ä»¥é€‰ \"all\" æˆ– \"lora_only\"ï¼Œä½†é€šå¸¸ \"none\" å°±å¤Ÿäº†ã€‚\n",
    "# task_type=\"CAUSAL_LM\"ï¼Œä»»åŠ¡ç±»å‹ã€‚\n",
    "#   å‘Šè¯‰ LoRA æˆ‘ä»¬åœ¨åšä¸€ä¸ªæ ‡å‡†çš„â€œå› æœè¯­è¨€æ¨¡å‹â€ä»»åŠ¡ï¼ˆå³ GPT å¼çš„æ–‡æœ¬ç”Ÿæˆï¼‰ã€‚\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], # Qwen å…¨çº¿æ€§å±‚å¾®è°ƒæ•ˆæœæœ€å¥½\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# 4. æ ¼å¼åŒ–å‡½æ•°ã€‚è¿™æ˜¯ä¸€ä¸ªå›è°ƒå‡½æ•°ï¼Œä¼šè¢« SFTTrainer è‡ªåŠ¨è°ƒç”¨ã€‚\n",
    "# å®ƒçš„ç›®çš„æ˜¯æŠŠæ•°æ®é›†ä¸­çš„åŸå§‹åˆ—ï¼ˆinstruction, thinking, outputï¼‰è½¬æ¢æˆæ¨¡å‹èƒ½çœ‹æ‡‚çš„ä¸€ä¸ªé•¿å­—ç¬¦ä¸²ã€‚\n",
    "\n",
    "# æ€ç»´é“¾èåˆï¼Œä»£ç å°† thinkingï¼ˆæ€ç»´é“¾/CoTï¼‰å’Œ outputï¼ˆæ ‡å‡†ç­”æ¡ˆï¼‰æ‹¼åœ¨äº†ä¸€èµ·ã€‚\n",
    "# è¿™æ„å‘³ç€æˆ‘ä»¬åœ¨æ•™æ¨¡å‹åœ¨å›ç­”é—®é¢˜å‰å…ˆè¾“å‡ºæ€è€ƒè¿‡ç¨‹ã€‚è¿™æ˜¯ç›®å‰ DeepSeek-R1 ç­‰æ¨ç†æ¨¡å‹çš„æ ¸å¿ƒè®­ç»ƒæ–¹å¼ã€‚\n",
    "\n",
    "# example å‚æ•°ï¼šè¿™æ˜¯ä¼ å…¥çš„ä¸€æ‰¹æ•°æ®ï¼ˆBatchï¼‰ï¼ŒåŒ…å«å¤šæ¡æ ·æœ¬ã€‚\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        # æ‹¼æ¥ \"æ€è€ƒè¿‡ç¨‹\" å’Œ \"æœ€ç»ˆç­”æ¡ˆ\"\n",
    "        output = \"[æ€è€ƒè¿‡ç¨‹]\\n\"+example[\"thinking\"][i]+\"\\n\\n[æœ€ç»ˆç­”æ¡ˆ]\\n\"+example[\"output\"][i] # è¿™é‡ŒåŒ…å«äº† Teacher ç”Ÿæˆçš„æ€ç»´é“¾\n",
    "        # æ„å»º Qwen æ ¼å¼æ–‡æœ¬\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": example['instruction'][i]},\n",
    "            {\"role\": \"assistant\", \"content\": output} # è¿™é‡Œçš„ output åŒ…å«æ€ç»´é“¾\n",
    "        ]\n",
    "        # åº”ç”¨åˆ†è¯å™¨çš„èŠå¤©æ¨¡æ¿\n",
    "        # æ ¹æ® Qwen æ¨¡å‹çš„è¦æ±‚ï¼Œè‡ªåŠ¨æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼ˆSpecial Tokensï¼‰ã€‚\n",
    "        #   ä¾‹å¦‚ï¼šå®ƒä¼šæŠŠ messages å˜æˆç±»ä¼¼è¿™æ ·çš„å­—ç¬¦ä¸²ï¼š\n",
    "        #   <|im_start|>system\\nYou are...<|im_end|><|im_start|>user\\né—®é¢˜...<|im_end|><|im_start|>assistant\\n[æ€è€ƒè¿‡ç¨‹]...\n",
    "        #   tokenize=Falseï¼šéå¸¸å…³é”®ã€‚æˆ‘ä»¬è¿™é‡Œåªç”Ÿæˆå­—ç¬¦ä¸²ï¼Œä¸è½¬æˆæ•°å­— IDã€‚å› ä¸º SFTTrainer å†…éƒ¨ä¼šç»Ÿä¸€åš Tokenizationã€‚\n",
    "        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "# 5. å…³é”®ï¼Œè¿™æ˜¯æŒ‡ä»¤å¾®è°ƒï¼ˆSFTï¼‰ä¸­æœ€é‡è¦çš„ä¸€ä¸ªç»„ä»¶ã€‚å®ƒçš„ä½œç”¨æ˜¯Maskï¼ˆæ©ç›–ï¼‰æ‰ç”¨æˆ·æé—®éƒ¨åˆ†çš„ Lossã€‚\n",
    "# è¿™ä¸ªç¥å™¨ä¼šè‡ªåŠ¨å¸®ä½ æŠŠ User çš„éƒ¨åˆ† mask æ‰ (è®¾ä¸º -100)ï¼Œåªè®¡ç®— Assistant å›å¤çš„ Loss\n",
    "\n",
    "# ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ\n",
    "#   å¦‚æœä½ ä¸åŠ è¿™ä¸ªï¼Œæ¨¡å‹åœ¨è®­ç»ƒæ—¶ä¼šåŒæ—¶è®¡ç®—â€œç”¨æˆ·é—®é¢˜â€å’Œâ€œAIå›ç­”â€çš„æŸå¤±ã€‚\n",
    "#   è¿™ä¼šå¯¼è‡´æ¨¡å‹é€šè¿‡èƒŒè¯µç”¨æˆ·çš„é—®é¢˜æ¥é™ä½ Lossï¼Œè€Œä¸æ˜¯å­¦ä¹ å¦‚ä½•å›ç­”ã€‚\n",
    "#   ä½¿ç”¨äº†è¿™ä¸ª Collator åï¼Œæ¨¡å‹åªä¼šè¢«æƒ©ç½šå›ç­”å¾—ä¸å¯¹çš„åœ°æ–¹ï¼Œè€Œä¸ä¼šè¢«æƒ©ç½šå¤è¿°é—®é¢˜çš„åœ°æ–¹ã€‚\n",
    "\n",
    "# å‚æ•°è¯¦ç»†è¯´æ˜ï¼š\n",
    "#   response_templateï¼Œåˆ†éš”ç¬¦ã€‚å‘Šè¯‰ Collator ä»å“ªé‡Œå¼€å§‹æ˜¯â€œAI çš„å›ç­”â€ã€‚\n",
    "#   <|im_start|>assistant\\nï¼šè¿™æ˜¯ Qwen æ¨¡å‹ç‰¹æœ‰çš„ç‰¹æ®Š Token åºåˆ—ï¼Œæ ‡å¿—ç€ Assistant å¼€å§‹è¯´è¯ã€‚\n",
    "#   Collator ä¼šåœ¨æ•´æ®µæ–‡æœ¬ä¸­å¯»æ‰¾è¿™ä¸ªå­—ç¬¦ä¸²ï¼ŒæŠŠè¿™ä¸ªå­—ç¬¦ä¸²ä¹‹å‰çš„æ‰€æœ‰ Tokenï¼ˆSystem Prompt + User Promptï¼‰çš„ Label è®¾ä¸º -100ï¼ˆPyTorch ä¸­å¿½ç•¥è®¡ç®— Loss çš„æ ‡è®°ï¼‰ã€‚\n",
    "# tokenizer=tokenizerï¼ŒCollator éœ€è¦åˆ†è¯å™¨æ¥æŠŠ response_template å­—ç¬¦ä¸²è½¬æˆ Token IDï¼Œä»¥ä¾¿åœ¨è½¬æ¢åçš„æ•°æ®ä¸­è¿›è¡ŒåŒ¹é…æŸ¥æ‰¾ã€‚\n",
    "response_template = \"<|im_start|>assistant\\n\" \n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "# 6. é…ç½®è®­ç»ƒå‚æ•°\n",
    "# ä½œç”¨ï¼ŒTrainingArguments æ˜¯ Hugging Face transformers åº“ä¸­ç”¨äºå®šä¹‰è®­ç»ƒè¿‡ç¨‹æ‰€æœ‰è¶…å‚æ•°ï¼ˆHyperparametersï¼‰çš„æ ¸å¿ƒç±»ã€‚\n",
    "# å®ƒè´Ÿè´£å‘Šè¯‰è®­ç»ƒå™¨ï¼ˆTrainerï¼‰â€œè¯¥æ€ä¹ˆç»ƒâ€â€”â€”æ¯”å¦‚ç»ƒå¤šå¿«ï¼ˆå­¦ä¹ ç‡ï¼‰ã€ç»ƒå¤šå°‘æ¬¡ï¼ˆEpochï¼‰ã€å å¤šå°‘æ˜¾å­˜ï¼ˆBatch Sizeï¼‰ã€å¾€å“ªé‡Œå­˜ï¼ˆOutput Dirï¼‰ç­‰ã€‚\n",
    "\n",
    "# output_dirï¼Œ æŒ‡å®šè¾“å‡ºç›®å½•ã€‚\n",
    "#   è®­ç»ƒè¿‡ç¨‹ä¸­ç”Ÿæˆçš„æ‰€æœ‰æ–‡ä»¶éƒ½ä¼šå­˜æ”¾åœ¨è¿™é‡Œã€‚åŒ…æ‹¬ï¼š\n",
    "#       Checkpointsï¼ˆæ£€æŸ¥ç‚¹ï¼‰ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¨¡å‹æƒé‡çš„ä¿å­˜ã€‚\n",
    "#       Logsï¼ˆæ—¥å¿—ï¼‰ï¼šLoss çš„å˜åŒ–è®°å½•ç­‰ã€‚\n",
    "#       Configï¼ˆé…ç½®ï¼‰ï¼šæœ€ç»ˆæ¨¡å‹çš„é…ç½®æ–‡ä»¶ã€‚\n",
    "\n",
    "# per_device_train_batch_sizeï¼Œå•å¼ æ˜¾å¡ä¸Šçš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚\n",
    "#   å®ƒå†³å®šäº†æ¯æ¬¡å‚æ•°æ›´æ–°å‰ï¼Œæ˜¾å¡ä¸€æ¬¡æ€§è¯»å…¥å¤šå°‘æ¡æ•°æ®è¿›è¡Œè®¡ç®—ã€‚\n",
    "#   æ•°å€¼å«ä¹‰ï¼šè¿™é‡Œè®¾ä¸º 8ï¼Œæ„å‘³ç€å¦‚æœä½ çš„æ˜¾å­˜å¤Ÿå¤§ï¼Œæ¯æ¬¡ä¼šåŒæ—¶æŠŠ 8 æ¡æ•°æ®å¡è¿›æ˜¾å¡è®¡ç®— Lossã€‚\n",
    "#   æ˜¾å­˜å½±å“ï¼šè¿™ä¸ªæ•°å€¼è¶Šå¤§ï¼Œè®­ç»ƒè¶Šå¿«ï¼Œä½†æ˜¾å­˜å ç”¨è¶Šé«˜ã€‚å¦‚æœæŠ¥ OOMï¼ˆæ˜¾å­˜æº¢å‡ºï¼‰é”™è¯¯ï¼Œé€šå¸¸ç¬¬ä¸€ä¸ªè¦æ”¹å°çš„å°±æ˜¯è¿™ä¸ªå‚æ•°ã€‚\n",
    "\n",
    "# gradient_accumulation_stepsï¼Œæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ã€‚\n",
    "#   è¿™æ˜¯å°æ˜¾å­˜è·‘å¤§ Batch Size çš„æ ¸å¿ƒæŠ€å·§ã€‚\n",
    "#   åŸç†ï¼šæ¨¡å‹ä¼šè¿›è¡Œ 4 æ¬¡å‰å‘ä¼ æ’­ï¼ˆForward Passï¼‰ï¼Œæ¯æ¬¡ç®—å®Œ Loss åä¸ç«‹å³æ›´æ–°æƒé‡ï¼Œè€Œæ˜¯æŠŠæ¢¯åº¦æ”’èµ·æ¥ã€‚ç­‰å‡‘å¤Ÿäº† 4 æ¬¡ï¼Œå†ä¸€æ¬¡æ€§æ›´æ–°æƒé‡ã€‚\n",
    "#   å®é™…/ç­‰æ•ˆ Batch Sizeï¼šTotal Batch Size=per_device_batch_sizeÃ—gradient_accumulation_stepsÃ—GPUæ•°é‡\n",
    "#   åœ¨æœ¬ä¾‹ä¸­ï¼š8Ã—4Ã—1=32\n",
    "#   è¿™æ„å‘³ç€è™½ç„¶å•æ¬¡åªè¯» 8 æ¡æ•°æ®ï¼Œä½†åœ¨æ•°å­¦ä¸Šç­‰åŒäºä¸€æ¬¡è¯»äº† 32 æ¡æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œä»è€Œä¿è¯äº†è®­ç»ƒçš„ç¨³å®šæ€§ã€‚\n",
    "\n",
    "# learning_rateï¼Œ å­¦ä¹ ç‡ï¼ˆOptimizer çš„æ­¥é•¿ï¼‰ã€‚\n",
    "# å†³å®šäº†æ¨¡å‹å‚æ•°æ¯æ¬¡æ›´æ–°å˜åŒ–çš„å¹…åº¦ã€‚\n",
    "# æ•°å€¼èƒŒæ™¯ï¼š2e-4ï¼ˆå³ 0.0002ï¼‰æ˜¯ LoRA å¾®è°ƒçš„ç»éªŒå€¼ã€‚\n",
    "# å¯¹æ¯”ï¼šå¦‚æœæ˜¯å…¨é‡å¾®è°ƒï¼ˆFull Fine-tuningï¼‰ï¼Œå­¦ä¹ ç‡é€šå¸¸å¾ˆå°ï¼ˆå¦‚ 1e-5 æˆ– 2e-5ï¼‰ã€‚ä½†å› ä¸º LoRA è®­ç»ƒå‚æ•°å¾ˆå°‘ï¼Œé€šå¸¸éœ€è¦æ›´å¤§çš„å­¦ä¹ ç‡æ‰èƒ½æ”¶æ•›ã€‚\n",
    "\n",
    "# logging_stepsï¼Œæ—¥å¿—æ‰“å°é¢‘ç‡ã€‚\n",
    "# è®¾ç½®æ¯è®­ç»ƒå¤šå°‘æ­¥ï¼ˆStepï¼‰å°±åœ¨æ§åˆ¶å°æ‰“å°ä¸€æ¬¡å½“å‰çš„ Loss å’Œå­¦ä¹ ç‡ã€‚\n",
    "# è®¾ä¸º 1 æ˜¯å› ä¸ºæ•°æ®é›†æå°åœ¨å¤§è§„æ¨¡è®­ç»ƒä¸­ï¼Œé€šå¸¸è®¾ç½®ä¸º 10 æˆ– 100ï¼Œå¦åˆ™å±å¹•ä¼šè¢«æ—¥å¿—åˆ·å±ï¼Œä¸”è½»å¾®å½±å“æ€§èƒ½ã€‚\n",
    "\n",
    "# num_train_epochs=3ï¼Œè®­ç»ƒè½®æ¬¡ï¼Œä¸€ä¸ªepochæ„å‘³ç€æ¨¡å‹æŠŠæ•´ä¸ªè®­ç»ƒé›†ä»å¤´åˆ°å°¾è·‘ä¸€é\n",
    "# è®¾ç½®ä¸º3æ˜¯å¾®è°ƒé¢†åŸŸçš„â€œé»„é‡‘æ ‡å‡†â€ï¼Œé€šå¸¸1-3ä¸ªepochå°±èƒ½å–å¾—å¾ˆå¥½çš„æ•ˆæœã€‚å†å¤šç»ƒå¯èƒ½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼Œå¤±å»æ³›åèƒ½åŠ›\n",
    "\n",
    "# save_strategy=\"epoch\", æ¨¡å‹ä¿å­˜çš„ç­–ç•¥ã€‚\n",
    "# å†³å®šäº†ä»€ä¹ˆæ—¶å€™ä¿å­˜ä¸€ä¸ªCheckjpoint(å­˜æ¡£)\n",
    "# epoch, è¡¨ç¤ºæ¯è·‘å®Œä¸€è½®ï¼Œå°±è‡ªåŠ¨ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "# stepsï¼Œ æ¯éš”Næ­¥ä¿å­˜ä¸€æ¬¡ï¼ˆé…åˆsave_stepså‚æ•°ï¼‰\n",
    "# no, è®­ç»ƒç»“é€Ÿå‰ä¸ä¿å­˜ä»»ä½•ä¸­é—´ç»“æœï¼ˆçœç¡¬ç›˜ï¼‰\n",
    "\n",
    "# report_to=\"none\"ï¼Œå¯è§†åŒ–æŠ¥å‘Šå·¥å…·é›†æˆã€‚\n",
    "# transformeråº“æ”¯æŒè‡ªåŠ¨æŠŠè®­ç»ƒæ•°æ®ä¸Šä¼ åˆ°WandBï¼ˆWeights & Biases), TensorBoard, MLflowç­‰å¹³å°\n",
    "# noneï¼Œ è¡¨ç¤ºå…³é—­ä¸Šä¼ åŠŸèƒ½ã€‚\n",
    "# åœ¨æœ¬åœ°å¼€å‘ã€æµ‹è¯•æˆ–è€…ä¸æƒ³æ³¨å†ŒWandBè´¦å·æ—¶ï¼Œè®¾ç½®ä¸ºnoneå¯ä»¥é¿å…å¾ˆå¤šç™»å½•æŠ¥é”™å’Œç½‘ç»œè¿æ¥é—®é¢˜\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=8, \n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3, \n",
    "    save_strategy=\"epoch\", \n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 7. å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "# SFTTrainerï¼ˆSupervised Fine-Tuning Trainerï¼‰æ˜¯ trl åº“æä¾›çš„æ ¸å¿ƒç±»ã€‚\n",
    "# å®ƒçš„ä½œç”¨æ˜¯å°†ä¹‹å‰å‡†å¤‡å¥½çš„æ¨¡å‹ã€æ•°æ®ã€è¶…å‚æ•°ã€å¾®è°ƒç­–ç•¥å…¨éƒ¨ç»„è£…åœ¨ä¸€èµ·ï¼Œæ„å»ºä¸€ä¸ªå¯ä»¥æ‰§è¡Œè®­ç»ƒä»»åŠ¡çš„å¯¹è±¡ã€‚\n",
    "# ç›¸æ¯”äº Hugging Face åŸç”Ÿçš„ Trainerï¼ŒSFTTrainer åšäº†å¾ˆå¤šé’ˆå¯¹æŒ‡ä»¤å¾®è°ƒï¼ˆInstruction Tuningï¼‰çš„è‡ªåŠ¨åŒ–å·¥ä½œï¼ˆå¦‚è‡ªåŠ¨åˆ†è¯ã€è‡ªåŠ¨åº”ç”¨ LoRAã€è‡ªåŠ¨å¤„ç† Dataset æ ¼å¼ç­‰ï¼‰ã€‚\n",
    "print(\"ğŸš€ å¼€å§‹é»‘ç›’è’¸é¦è®­ç»ƒ (SFT)...\")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 8. ä¿å­˜ä½ çš„æˆæœ\n",
    "print(f\"ğŸ‰ è®­ç»ƒå®Œæˆï¼ä¿å­˜é€‚é…å™¨åˆ° {OUTPUT_DIR}\")\n",
    "trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051d3037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== âš”ï¸ æ•ˆæœå¯¹å†³ âš”ï¸ ===\n",
      "\n",
      "ğŸ”´ åŸå§‹æ¨¡å‹å›ç­”:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "ä¸‰è§’å½¢çš„åº•æ˜¯8ï¼Œé«˜æ˜¯3ï¼Œæ±‚å…¶é¢ç§¯æ˜¯å¤šå°‘\n",
      "assistant\n",
      "ä¸‰è§’å½¢çš„é¢ç§¯å¯ä»¥é€šè¿‡å…¬å¼è®¡ç®—å¾—å‡ºã€‚å¯¹äºä¸€ä¸ªç­‰è…°ä¸‰è§’å½¢ï¼ˆå³æœ‰ä¸¤ä¸ªç›¸ç­‰çš„è¾¹ï¼‰ï¼Œå…¶é¢ç§¯å…¬å¼ä¸ºï¼š\n",
      "\n",
      "\\[ \\text{é¢ç§¯} = \\frac{1}{2} \\times \\text{åº•} \\times \\text{é«˜} \\]\n",
      "\n",
      "ç»™å®šçš„ä¸‰è§’å½¢åº•æ˜¯8ï¼Œé«˜æ˜¯3ï¼Œä»£å…¥ä¸Šè¿°å…¬å¼å¾—ï¼š\n",
      "\n",
      "\\[ \\text{é¢ç§¯} = \\frac{1}{2} \\times 8 \\times 3 = 4 \\times 3 = 12 \\]\n",
      "\n",
      "æ‰€ä»¥ï¼Œè¿™ä¸ªä¸‰è§’å½¢çš„é¢ç§¯æ˜¯12å¹³æ–¹å•ä½ã€‚\n",
      "\n",
      "Loading LoRA adapters...\n",
      "ğŸŸ¢ è’¸é¦æ¨¡å‹å›ç­”:\n",
      "system\n",
      "You are a helpful assistant.\n",
      "user\n",
      "ä¸‰è§’å½¢çš„åº•æ˜¯8ï¼Œé«˜æ˜¯3ï¼Œæ±‚å…¶é¢ç§¯æ˜¯å¤šå°‘\n",
      "assistant\n",
      "[æ€è€ƒè¿‡ç¨‹]\n",
      "ä¸‰è§’å½¢çš„é¢ç§¯è®¡ç®—å…¬å¼æ˜¯åº•ä¹˜ä»¥é«˜å†é™¤ä»¥2ã€‚é¦–å…ˆï¼Œç¡®å®šåº•è¾¹é•¿åº¦ä¸º8å˜ç±³ï¼Œé«˜ä¸º3å˜ç±³ã€‚ç„¶åï¼Œå°†åº•å’Œé«˜ç›¸ä¹˜å¾—åˆ°8ä¹˜ä»¥3ç­‰äº24ã€‚æœ€åï¼Œå°†ç»“æœé™¤ä»¥2ï¼Œå¾—åˆ°é¢ç§¯ä¸º12å¹³æ–¹å˜ç±³ã€‚\n",
      "\n",
      "[æœ€ç»ˆç­”æ¡ˆ]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "# 1. å‡†å¤‡æµ‹è¯•é—®é¢˜ (ç”¨è®­ç»ƒè¿‡çš„é—®é¢˜æµ‹è¯•è¿‡æ‹Ÿåˆï¼Œç”¨æ–°é—®é¢˜æµ‹è¯•æ³›åŒ–)\n",
    "#test_question = \"å¦‚æœä½ æœ‰3ä¸ªè‹¹æœï¼Œåƒæ‰äº†1ä¸ªï¼Œåˆä¹°æ¥äº†5ä¸ªï¼Œç°åœ¨æœ‰å‡ ä¸ªï¼Ÿ\" \n",
    "test_question = \"ä¸‰è§’å½¢çš„åº•æ˜¯8ï¼Œé«˜æ˜¯3ï¼Œæ±‚å…¶é¢ç§¯æ˜¯å¤šå°‘\"\n",
    "# 2. å®šä¹‰æ¨ç†å‡½æ•°\n",
    "def run_inference(model, tokenizer, question):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    outputs = model.generate(**inputs, max_new_tokens=256, temperature=0.1) # ä½æ¸©ä»¥ä¾¿å¤ç°\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\n=== âš”ï¸ æ•ˆæœå¯¹å†³ âš”ï¸ ===\\n\")\n",
    "\n",
    "# --- å›åˆ 1: åŸå§‹å‚»ç“œæ¨¡å‹ (Base Student) ---\n",
    "# é‡æ–°åŠ è½½çº¯å‡€æ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_ID, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "print(f\"ğŸ”´ åŸå§‹æ¨¡å‹å›ç­”:\\n{run_inference(base_model, tokenizer, test_question)}\")\n",
    "\n",
    "# --- å›åˆ 2: è’¸é¦åçš„æ¨¡å‹ (Distilled Student) ---\n",
    "# åŠ è½½åˆšæ‰è®­ç»ƒå¥½çš„ LoRA\n",
    "print(\"\\nLoading LoRA adapters...\")\n",
    "distilled_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
    "print(f\"ğŸŸ¢ è’¸é¦æ¨¡å‹å›ç­”:\\n{run_inference(distilled_model, tokenizer, test_question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb47cf10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
